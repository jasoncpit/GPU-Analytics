[{"id":0,"href":"/GPU-Analytics/docs/case_study/chapter_1/","title":"Chapter 1 - Address geocoding","section":"Case Studies","content":" Address geocoding # Address geocoding, or address matching, is an example of a broad class of data science problems known as data linkage. Data linkage problems involve connecting two datasets together by establishing an association between two records based on one or more common properties. In the case of address geocoding, one would try to assign XY coordinates to a list of address strings derived from, for instance, consumer sources that are not explicitly georeferenced (see, for instance, Lansley et al. 2019). The process involves linking the address list to a authoritative reference database that contains address strings with their associated coordinates. However, due to the lack of a universal standard on how addresses are structured and stored, matching addresses becomes a significantly complicated task that requires a great deal of data preparation and processing (see Table 1).\nInput address string Reference data to match against Unstructured text Structured, tokenised Messy, containing typos and abbreviations Clean, standardised, (mostly) correct Incomplete Snapshot of addresses at a given time Range from historic to very recent addresses, including businesses Organisation / business names are not always part of the address Table 1: Summary of address matching challenges (Office for National Statistics, 2022).\nObjective # Not only is address matching problem a complex problem, its complexity increases exponentially with an increase in data sizes. The objective of this first Case Study, therefore, is to showcase how we can utilise a GPU\u0026rsquo;s processing capabilities for address matching - and data linkage more general.\nAddress matching pipeline # The address matching process can be split into three high-level steps: data pre-processing, candidate address retrieval, and candidate scoring and ranking.\nAddress string-preprocessing # At the most fundamental level, we need to prepare the data for the matching process. There are potentially different approach to do this, but the most common approach is to concatenate (join) the address into its constituent parts if this is not the case already. Alternatively, the input address can be split into corresponding parts, such as the street name, house number, postcode, and so on. The former approach is more common, but it ignores the information in the data about the address, and makes it impossible to rely on the natural structure of the address to help match the desired address with the input string. The latter approach is more complex, but flexible, it allows for more accurate comparison because comparing tokens precludes the possibility of the same word representing one element of the address being compared against an unrelated element of the address.\nCandidate address: retrieval # In the second step, we need a method to compare the input address with the reference data. The simplest approach is to compare each token of the input address / each address string with each token / each address string of the reference data. This approach is simple, but it cannot deal with typos and abbreviations. A common solution to deal with this is by using a probabilistic data matching algorithm, such as fuzzy string matching with similarity measures to accommodate typos and misspellings: to make sure that, for instance, \u0026ldquo;Birmingam\u0026rdquo; would match \u0026ldquo;Birmingham\u0026rdquo;. However, in practice, when each record is compared against every other record using some type of similarity measure (e.g. the Levenshtein distance), the number of comparisons can be very large, thus leading to a very expensive computation that cannot effectively be deployed on a GPU. Instead, we can consider using a Natural Language Processing (NLP) approach to compare the addresses by assuming that the address is a sequence of unstructured text. One of the most common approaches is to convert the address into a vector and then compare the vector similarity (e.g., TF-IDF) to select potentially matching candidates.\nCandidate address: scoring and ranking # The last step in the process is to evaluate the quality of the match between the input address string and the retrieved candidate addresses. The most common approach is to use a similarity score to evaluate the similarity among all potential candidate addresses. Depending on the application, the similarity score can be a simple percentage or a more complex score. For instance, we can define a threshold for the similarity score, and only return the candidate addresses that have a similarity score above the threshold. We can also evaluate the model performance by validating the results with a ground truth dataset.\nGPU Considerations # The address matching problem is a computationally intensive problem. A core challenge is to understand which part of the process is the most computationally intensive and which part of the process can be efficiently parallelized on a GPU. In the first part, we can concatenate the address from its constituent parts, whereby treating each address as a sequence of unstructured text. In the second part, we can use a character-based n-gram TF-IDF to convert the address into a vector. While the terms in TF-IDF are usually words, this is not a requirement. We can use n-grams, sequences of N continuous characters, to convert the address into a vector representation based on the character level. For example:\n#Input text = \u0026#34;Birmingham\u0026#34; #Create a list of n-grams n_grams = [text[i:i+3] for i in range(len(text)-2+1)] print(n_grams) #[\u0026#39;Bir\u0026#39;, \u0026#39;irm\u0026#39;, \u0026#39;rmi\u0026#39;, \u0026#39;min\u0026#39;, \u0026#39;ing\u0026#39;, \u0026#39;ngh\u0026#39;, \u0026#39;gha\u0026#39;, \u0026#39;ham\u0026#39;] In the third part, we can use a similarity score to evaluate the similarity among all potential candidate addresses; for instance by ranking the candidate addresses based on the similarity score.\nUsing TF-IDF with n-grams as terms to find similar strings transforms the problem into a matrix multiplication problem, which is computationally much cheaper. This approach can significantly reduce the memory it takes to compare strings in comparison to a fuzzy string matching algorithm with TF-IDF and a nearest neighbors algorithm. More importantly, using a GPU for the matrix multiplication can further speed up the string comparison.\nCase Study # Example data # In this tutorial, we will be using two US hospital data sets. The first is a data set that contains hospital banking details (see Table 2). The second data set contains information on payments hospitals have received from the insurance provider (see Table 3). We would like to linkt these two datasets for further analysis, however, unfortunately, they currently are not linked.\nAccount_Num Facility Name Address City State ZIP Code County Name Phone Number Hospital Type Hospital Ownership 0 10605 SAGE MEMORIAL HOSPITAL STATE ROUTE 264 SOUTH 191 GANADO AZ 86505 APACHE (928) 755-4541 Critical Access Hospitals Voluntary non-profit - Private 1 24250 WOODRIDGE BEHAVIORAL CENTER 600 NORTH 7TH STREET WEST MEMPHIS AR 72301 CRITTENDEN (870) 394-4113 Psychiatric Proprietary Table 2: Sample of the account hospital data\nProvider_Num Provider Name Provider Street Address Provider City Provider State Provider Zip Code Total Discharges Average Covered Charges Average Total Payments Average Medicare Payments 0 839987 SOUTHEAST ALABAMA MEDICAL CENTER 1108 ROSS CLARK CIRCLE DOTHAN AL 36301 118 20855.6 5026.19 4115.52 1 519118 MARSHALL MEDICAL CENTER SOUTH 2505 U S HIGHWAY 431 NORTH BOAZ AL 35957 43 13289.1 5413.63 4490.93 Table 3: Sample of the reimbursement data\nBelow we compare the speed of computing TF-IDF using sklearn (CPU) and RAPIDS\u0026rsquo; cuML (GPU) module. To make the difference more distinguishable, we will test how the increase of vocabulary size would affect the performance in multiple runs.\nSetting up # We start by importing the libraries that we will need, loading the individual data sets, and concatenating the individual address components in each of the data sets.\n#Import libraries import cudf import pandas as pd from cuml.feature_extraction.text import TfidfVectorizer as GPU_TfidfVectorizer from sklearn.feature_extraction.text import TfidfVectorizer as CPU_TfidfVectorizer import time import numpy as np import seaborn as sns import matplotlib.pyplot as plt #Load data data1_url = \u0026#34;https://raw.githubusercontent.com/chris1610/pbpython/master/data/hospital_account_info.csv\u0026#34; data2_url = \u0026#34;https://raw.githubusercontent.com/chris1610/pbpython/master/data/hospital_reimbursement.csv\u0026#34; account = pd.read_csv(data1_url) #Hospital account information reimbursement = pd.read_csv(data2_url) #Hospital reimbursement information #Converting facility name, address, city and state into one string account_full_address = account.apply(lambda x: \u0026#34; \u0026#34;.join([x[\u0026#39;Facility Name\u0026#39;],x[\u0026#39;Address\u0026#39;],x[\u0026#39;City\u0026#39;],x[\u0026#39;State\u0026#39;]]), axis=1).tolist() #Converting facility name, address, city and state into one string reimbursement_full_address = reimbursement.apply(lambda x: \u0026#34; \u0026#34;.join([x[\u0026#39;Provider Name\u0026#39;],x[\u0026#39;Provider Street Address\u0026#39;],x[\u0026#39;Provider City\u0026#39;],x[\u0026#39;Provider State\u0026#39;]]), axis=1).tolist() TF-IDF vectorisation # Now we are set-up, we can assess the impact of data size on computational time for both our CPU and GPU approach. To allow for a fair comparison between the CPU and GPU, we will be using the same data set and increase the size by the data set by x times on each run.\n#Inititate sklearn vectoriser and cuml vectosiser #CPU vectorizer from sklearn cpu_vectorizer = CPU_TfidfVectorizer(analyzer=\u0026#39;char\u0026#39;,ngram_range=(1,2)) #GPU vectorizer from cuml gpu_vectorizer = GPU_TfidfVectorizer(analyzer=\u0026#39;char\u0026#39;,ngram_range=(1,2)) #analyzer=\u0026#39;char\u0026#39; means we are using character as the input #ngram_range = (1,2) means we are looking at both unigram and bigram for the model input #Manually inflating number of rows with 10 run times total_datasize = [] cpu_time = [] gpu_time = [] for run in range(1,10): for i in range(1,50): #Manually inflating the number of records input = reimbursement_full_address*i total_datasize.append(len(input)) #Cpu runtime -------------------------------- start = time.time() cpu_output = cpu_vectorizer.fit_transform(input) done = time.time() elapsed = done - start cpu_time.append(elapsed) #gpu runtime -------------------------------- start = time.time() #Convert input to cudf series gpu_output = gpu_vectorizer.fit_transform(cudf.Series(input)) done = time.time() elapsed = done - start gpu_time.append(elapsed) #Create a dataframe to store the results gpu_elapsed = pd.DataFrame({\u0026#34;time\u0026#34;:gpu_time,\u0026#34;data_size\u0026#34;:total_datasize,\u0026#39;label\u0026#39;:\u0026#34;gpu\u0026#34;}) cpu_elapsed = pd.DataFrame({\u0026#34;time\u0026#34;:cpu_time,\u0026#34;data_size\u0026#34;:total_datasize,\u0026#39;label\u0026#39;:\u0026#34;cpu\u0026#34;}) result = pd.concat([gpu_elapsed,cpu_elapsed]).reset_index() #Plot results fig, ax = plt.subplots(figsize=(10,10)) sns.lineplot(x= \u0026#39;data_size\u0026#39;,y=\u0026#39;time\u0026#39;,hue = \u0026#39;label\u0026#39;,data = result,ax = ax ) plt.xlabel(\u0026#39;Data Size\u0026#39;) plt.ylabel(\u0026#34;Time Elapsed \u0026#34;) plt.title(\u0026#34;Comparing the speed of TF-IDF vectorisation on CPU and GPU\u0026#34;) plt.show() print(cpu_output.shape) #(25000, 874) -\u0026gt; meaning we have 24273 rows of address and 874 characters in the vocabulary as input Figure 1: Comparing the speed of TF-IDF vectorisation on CPU and GPU. There are few things we can observe in the code above. First, the code between the CPU and GPU is almost identical. The only difference is that we are using the functions from different libraries. Second, we can see that the GPU is able to process the data much faster than the CPU. On average, it takes less than 0.1 seconds to process around 25 thousand addresses on the GPU. In comparison, it takes 1 second on the CPU for the same input dataset. Furthermore, we can also observe that the run time of CuML’s TfidfVectorizer is almost constant as the input data size increases, whereas the run time of scikit-learn’s TfidfVectorizer grows exponentially. This is because the GPU is able to process the data in parallel, which makes it much more efficient than the CPU.\nCosine similarity # Now that we have our TF-IDF matrix, we want to use the hospital reimbursement data to find the most relevant address from the hospital data. To do this, we can find the address with the smallest distance (or highest similarity) to our query address. For the second step, we will compare the speed of computing the cosine similarity using the CPU and GPU. We do this by comparing the computation time of calculating the pair-wise cosine similarity between two matrices using NumPy, CuPy and Numba from scratch. We will use the TF-IDF matrix we created from the reimbursement data in the previous step as the input, and hospital data as the target.\n#Vectorize the target address cpu_target =cpu_vectorizer.transform(account_full_address) gpu_target = gpu_vectorizer.transform(cudf.Series(account_full_address)) #Import libraries import cupy as cp import numpy as np import scipy import gc import torch from cupyx.scipy.sparse.linalg import norm as cp_norm from scipy.sparse.linalg import norm as sc_norm #Numpy def np_cosine_similarity(query, target): # Assert that the input matrices have the same number of columns assert(query.shape[1] == target.shape[1]) #Calculate the dot product dot_product = np.dot(query,target.T) #Calculate l2 norm for query and target query_norm = sc_norm(query,axis=1) target_norm = sc_norm(target,axis=1) return dot_product/(query_norm[:,np.newaxis]*target_norm[:,np.newaxis].T) #Cupy def cp_cosine_similarity(query, target): # Assert that the input matrices have the same number of columns assert(query.shape[1] == target.shape[1]) #Initiate GPU instance with cp.cuda.Device(0): #Check whether the sparse matrix is compatible with Cupy, if not then convert if isinstance(query,scipy.sparse._csr.csr_matrix) and isinstance(target,scipy.sparse._csr.csr_matrix): #Convert the input matrices to sparse format and copy to the GPU query = cp.sparse.csr_matrix(query, copy=True) target = cp.sparse.csr_matrix(target, copy=True) #Dot product using cupy.dot() dot_product = query.dot(target.T) #Calculate l2 norm for query and target query_norm = cp_norm(query,axis=1) target_norm = cp_norm(target,axis=1) #Compute the cosine similarity output = dot_product / (cp.expand_dims(query_norm,axis=1) * cp.expand_dims(target_norm,axis=0)) #Converting back to numpy array result = output.get() return result From the above code, we can see that again the differences between NumPy and CuPy are minimal. Most mathematics functions in CuPy such as calculating the dot products and l2 norm have the same API as NumPy and SciPy. However, there are few things worth noticing. Firstly, when using CuPy, we have to manually initiate the GPU instance by calling function with cp.cuda.Device(0): and then execute your calculation. This allows to temporarily switching the currently active GPU device. Secondly, we need to manually specify the data type and transfer the data on the GPU instance by using cp.sparse.csr_matrix(...,copy = True) . This is to ensure we can properly prepare the data types for CUDA operation (for more information on the supporting data types, please refer to the CuPy documentation). And lastly, to get the data from the GPU, we need to use .get() to return a copy of the array on host memory.\nNumba [Optional] # We can actually go a step further, we can actually explicitly write computing kernels with Numba. Numba is a Just-In-Time (JIT) compiler for Python that can take functions with numerical values or arrays and compile them to assembly, so they run at high speed. One advantage of writing with Numba is that we can write the low-level code in Python and then integrate it with CUDA assembly. In contrast to CuPy, Numba offers some flexibility in terms of the data types and kernels that can be used. However, Numba does not yet implement the full CUDA API, so some features are not available. It is worth noting here the goal of this post is to demonstrate the use of Numba and not to provide a comprehensive guide on how to use Numba. If you are interested in learning more about Numba, see the Numba documentation. The following code shows an example of the implementation of the cosine similarity function using Numba.\n#Numba ---------------------------------------------------------------- from numba import cuda #Define the kernel for calculation @cuda.jit #compiler decorator def pairwise_cosine_similarity(A, B, C): i, j = cuda.grid(2) #use 1 for x if i \u0026lt; C.shape[0] and j \u0026lt; C.shape[1]: dot = 0.0 normA = 0.0 normB = 0.0 for k in range(A.shape[1]): a = A[i, k] b = B[j, k] dot += a * b normA += a ** 2 normB += b ** 2 C[i, j] = dot / (normA * normB)**0.5 def Numba_cuda_cosine_similarity(query,target): #Assert that the input matrices have the same number of columns assert(query.shape[1] == target.shape[1]) #Allocate memory on the device for the result output = cuda.device_array((query.shape[0],target.shape[0])) #Check whether the sparse matrix is compatible with numba, if not then convert if isinstance(query,scipy.sparse._csr.csr_matrix) and isinstance(target,scipy.sparse._csr.csr_matrix): #Convert the input matrices to numpy array and copy to the GPU query = cuda.to_device(query.toarray()) target = cuda.to_device(target.toarray()) #Set the number of threads in a block ----- threadsperblock = (32,32) #Calculate the number of thread blocks in the grid blockspergrid_x = (output.shape[0] + (threadsperblock[0] - 1)) // threadsperblock[0] blockspergrid_y = (output.shape[1] + (threadsperblock[1] - 1)) // threadsperblock[1] blockspergrid = (blockspergrid_x, blockspergrid_y) #Starting the kernel pairwise_cosine_similarity[blockspergrid, threadsperblock](query, target, output) #Copy the result back to the host return output.copy_to_host() Let\u0026rsquo;s break down the code. @cuda.jit is a decorator, and it defines functions which will compile on the GPU (kernels). The pairwise_cosine_similarity function defines the steps of calculation inside the kernel. The cuda.grid function returns the 2D grid indices for the current thread executing in a CUDA kernel. The CUDA programming model uses a grid of threads to perform parallel computation, where each thread operates on a different portion of the input data. In the example code, the cuda.grid function is used to determine the indices of the current thread in the grid, represented as i and j in the code. These indices are used to access the corresponding row of the input matrices A and B, and the corresponding location in the output matrix C. In essence, the cuda.grid function is used to ensure that each thread operates on a different portion of the input data and outputs its result to the corresponding location in the output matrix, thus enabling the parallel computation of the cosine similarity. Furthermore, we need to manually define the shape of matrix C, and allocate memory on the device for the result. This is because the kernels cannot return numerical values, so we can get around that by passing inputs and outputs.\nIn the Numba_cuda_cosine_similarity function, we follow the same logic as with the CuPy function, however, we have to manually define the number of threads in a block and calculate the number of thread blocks in the grid. The threadsperblock defines a group of threads that can execute concurrently. A single thread represents the smallest unit of execution in a CUDA kernel. Each thread operates on a different portion of the input data and computes its result. The results computed by each thread are then combined to produce the final result. In general, the number of threads per block is a trade-off between performance and memory usage. Increasing the number of threads per block can increase performance by allowing more parallel computation, but it also increases the memory usage of the block. The GPU has a limited amount of shared memory that is shared by all threads in a block, and increasing the number of threads per block can cause the shared memory usage to exceed the available memory. The value of 32, 32 for threadsperblock is a common choice because it is a relatively small number of threads that can still provide good performance, while minimising the memory usage of the block. However, the optimal value for threadsperblock depends on the specific requirements of the computation and the hardware being used, and it may need to be adjusted for different computations or hardware.\nNext we define the number of thread blocks in the grid with the variable blockspergrid. The number of thread blocks in the grid is defined by the number of threads in each dimension of the block, and the number of rows and columns in the output matrix. The purpose of adding (threadsperblock[0] - 1) to C.shape[0] is to ensure that the integer division will round up to the nearest whole number, rather than rounding down. In the last step, we can invoke the kernel by the defined block size and thread size and passing the input matrices and the output matrix to the kernel function. The kernel function will then perform the computation in parallel on the GPU. After the computation is finished, we can copy the result back to the host. The code below compares the Numba implementation to the CuPy (GPU) and sklearn (CPU) implementations.\n#Result placeholders total_datasize = [] cpu_time = [] gpu_time = [] numba_time = [] for size in tqdm(range(1000,cpu_output.shape[0],5000)): total_datasize.append(size) query = cpu_output[0:size,] #CPU calculation start = time.time() _ = np_cosine_similarity(query,cpu_target) done = time.time() elapsed = done - start cpu_time.append(elapsed) #GPU calculation start = time.time() _ = cp_cosine_similarity(query,cpu_target) done = time.time() elapsed = done - start gpu_time.append(elapsed) #Numba CUDA ---- start = time.time() _ = Numba_cuda_cosine_similarity(query,cpu_target) done = time.time() elapsed = done - start numba_time.append(elapsed) #Plot fig, ax = plt.subplots(figsize=(10,10)) gpu_elapsed = pd.DataFrame({\u0026#34;time\u0026#34;:gpu_time,\u0026#34;data_size\u0026#34;:total_datasize,\u0026#39;label\u0026#39;:\u0026#34;gpu\u0026#34;}) cpu_elapsed = pd.DataFrame({\u0026#34;time\u0026#34;:cpu_time,\u0026#34;data_size\u0026#34;:total_datasize,\u0026#39;label\u0026#39;:\u0026#34;cpu\u0026#34;}) numba_elasped = pd.DataFrame({\u0026#34;time\u0026#34;:numba_time,\u0026#34;data_size\u0026#34;:total_datasize,\u0026#39;label\u0026#39;:\u0026#34;numba\u0026#34;}) result = pd.concat([gpu_elapsed,cpu_elapsed,numba_elasped]).reset_index() sns.lineplot(x= \u0026#39;data_size\u0026#39;,y=\u0026#39;time\u0026#39;,hue = \u0026#39;label\u0026#39;,data = result,ax = ax ) plt.xlabel(\u0026#39;Data Size\u0026#39;) plt.ylabel(\u0026#34;Time Elapsed \u0026#34;) plt.title(\u0026#34;Comparing the speed of matrix multiplication on CPU and GPU\u0026#34;) plt.show() Figure 2: Comparing the speed of matrix multiplication on CPU and GPU. Again we can see a major speed difference. Overall, the operations on GPU are much faster than the ones on CPU. Specifically, by leveraging JIT compilation and Numba CUDA, we can reduce the time of matrix multiplication from 10.8 seconds to around 1 second. The code above is a demonstration of matrix multiplication, but in practice, we can use functions such as cosine_similarity from sklearn.metrics.pairwise or sparse_pairwise_distances from cuml.metrics to calculate the cosine similarity between two matrices.\nSimilarity scores # Now we know how to run and speed up string matching, we can have a look at the actual results of the task. In this step, we will use the address with the highest cosine similarity result to find the best match for each query. Since we do not have a ground truth, we will be using the zip code as a proxy to check the accuracy of the model.\n#Import libraries from collections import defaultdict #Run analysis pipeline on the GPU reimbursement_tfidf= gpu_vectorizer.fit_transform(cudf.Series(reimbursement_full_address)) account_tfidf = gpu_vectorizer.transform(cudf.Series(account_full_address)) similarity_matrix = Numba_cuda_cosine_similarity(reimbursement_tfidf,account_tfidf) result = defaultdict(list) #Getting the reimbursement address and state, account address and state for index in range(similarity_matrix.shape[0]): most_similar_index = similarity_matrix[index].argmax() result[\u0026#39;reimbursement_address\u0026#39;].append(reimbursement_full_address[index]) result[\u0026#39;account_address\u0026#39;].append(account_full_address[most_similar_index]) result[\u0026#39;reimbursment_zip\u0026#39;].append(reimbursement.loc[index,\u0026#39;Provider State\u0026#39;]) result[\u0026#39;account_zip\u0026#39;].append(account.loc[most_similar_index,\u0026#39;State\u0026#39;]) result[\u0026#39;similarity_score\u0026#39;].append(similarity_matrix[index][most_similar_index]) result_df = pd.DataFrame(result) print(result_df.sort_values(\u0026#39;similarity_score\u0026#39;,ascending=True).head(2) print(result_df.sort_values(\u0026#39;similarity_score\u0026#39;,ascending=False).head(2) reimbursement_address account_address reimbursment_zip account_zip similarity_score 1325 UNITY HOSPITAL 550 OSBORNE ROAD FRIDLEY MN UNITY HOSPITAL OF ROCHESTER 1555 LONG POND ROAD ROCHESTER NY 55432 14626 0.551186 1634 TLC HEALTH NETWORK 845 ROUTES 5 AND 20 IRVING NY STANDING ROCK INDIAN HEALTH SERVICE HOSPITAL 10 NORTH RIVER ROAD FORT YATES ND 14081 58538 0.555283 Table 4: Examples of matched records with the lowest cosine similarity score.\nreimbursement_address account_address reimbursment_zip account_zip similarity_score 2242 BAPTIST MEMORIAL HOSPITAL UNION CITY 1201 BISHOP ST, PO BOX 310 UNION CITY TN BAPTIST MEMORIAL HOSPITAL UNION CITY 1201 BISHOP ST, PO BOX 310 UNION CITY TN 38261 38261 1 89 BANNER DESERT MEDICAL CENTER 1400 SOUTH DOBSON ROAD MESA AZ BANNER DESERT MEDICAL CENTER 1400 SOUTH DOBSON ROAD MESA AZ 85202 85202 1 Table 5: Examples of matched records with the highest cosine similarity score.\nThe tables show that the matched records with the lowest cosine similarity score are not the same, but the matched records with the highest cosine similarity score are almost identical. This is a good sign that the model is working as expected, we still need the inspect the accuracy scores in a bit more detail.\n#Import libraries from sklearn.metrics import accuracy_score #Get accuracy scores print(accuracy_score(result_df.reimbursment_zip,result_df.account_zip)) The accuracy score is 0.97, which means that 97% of the time, the model identifies the best match within the same zip code. Furthermore, by inspecting the distribution of the similarity score (see Figure 3), we can see that majority of matched records have cosine similarity scores above 0.95. However, there are some records that have a similarity score below 0.5, this may be due to the absence of such address from the hospital account holders.\n#Import libraries import matplotlib.pyplot as plt #Plot fig,ax = plt.subplots(figsize=(10,10)) result_df.similarity_score.plot(kind=\u0026#39;hist\u0026#39;,bins=10,ax=ax) plt.xlabel(\u0026#39;Cosine Similarity Score\u0026#39;) plt.title(\u0026#39;Distribution of cosine similarity score\u0026#39;) Figure 3: Distribution of cosine similarity score among matched records. Conclusion # In this case study, we explored the potential of GPUs to speed up explicit data linkage process that rely on string comparisons. By implementing vectorisation and cosine similarity calculation on a GPU, we were able to significantly enhance the performance of the process. More importantly, we can see that the functions and code can rather easily be ported to a GPU without too much modification.\nMore general, however, address matching is a complex problem that involves many factors that can impact the accuracy of the model. Some of these challenges include the presence of special characters, abbreviations, changes and absence of unique identifiers in the data. Often the task of data linkage is not a one-time process, but rather an iterative process that requires constant monitoring on the performance of the model and making adjustments accordingly. GPU can be a great tool to help speed up such processes.\nOne last thing to be aware of, particularly when dealing with large datasets, is the memory limitation of the GPU. One might consider using a GPU in conjunction with a scaling tool such as DASK, which can distribute the computation across multiple machines and servers. This approach can help mitigate the GPU memory limitation that can arise when processing larger data sets. Additionally, we can consider the batch size or the number of records that can be processed at a time to ensure that the GPU\u0026rsquo;s memory is not overloaded.\n"},{"id":1,"href":"/GPU-Analytics/docs/case_study/chapter_2/","title":"Chapter 2 - GeoAI and Deep Learning","section":"Case Studies","content":" GeoAI and Deep Learning # GeoAI, or geospatial artificial intelligence (AI), has become a trending topic and the frontier for spatial analytics in Geography (Li and Hsu, 2022). Although the field of AI has experienced highs and lows in the past decades, it has recently gained tremendous momentum because of breakthrough developments in deep (machine) learning, immense available computing power, and the pressing needs for mining and understanding big data.\nObjectives # The objective of the second Case Study is to showcase how we can use GPU for satellite image classification. We will be discussing two case studies - (1) training a CNN model from scratch using Pytorch to detect land use classification from satellite images (2) comparing model performance with a fine-tuned VGG16 model.\nWhile using a GPU is a commonly integrated into deep learning libraries, we will also provide best practices for maximizing your training efficiency.\nCase Study 1: Classifying EuraSat images using Convolutional Neural Networks (CNNs) # In this case study, we will be using the EuraSat dataset to train a CNN model to classify land use from satellite images. The EuraSat dataset contains 27,000 images of 10 different land use classes. The dataset is available on Kaggle and can be downloaded here. The dataset is also available on the PyTorch website.\nBrief introduction to Convolutional Neural Networks (CNNs) # Convolutional Neural Networks (CNNs) are a type of artificial neural network that are designed to work with grid-structured data, such as an image, a speech signal, or a video. They are particularly effective for image and video classification, object detection and recognition, and natural language processing tasks.\nThe key components of a CNN are convolutional layers, activation functions, pooling layers, and fully connected layers.\nConvolutional layers: Convolutional layers are the building blocks of a CNN. They perform a convolution operation on the input data, where a small matrix (known as a filter or kernel) is moved across the input data, element-wise multiplication is performed between the elements of the filter and the input data, and then the results are summed up to produce a single output value. This process is repeated for every possible position of the filter, resulting in a set of outputs, called feature maps. Convolutional layers can extract features from the input data, such as edges, shapes, textures, etc.\nActivation functions: Activation functions are used to introduce non-linearity into the network. They are applied element-wise to the output of the convolutional layer. The most commonly used activation functions in CNNs are Rectified Linear Unit (ReLU) and sigmoid.\nPooling layers: Pooling layers are used to reduce the spatial size of the feature maps, making the network less computationally expensive and more robust to changes in the position of objects in the input data. There are several types of pooling, including max pooling and average pooling. In max pooling, the maximum value in a region of the feature map is taken as the output, while in average pooling, the average value in a region is taken as the output.\nFully connected layers: The fully connected layers are used to make the final prediction using the features extracted by the convolutional and pooling layers. They perform a weighted sum of the inputs, followed by a non-linear activation function, and then produce the final output of the network.\nThe architecture of a CNN can be designed for a specific task by choosing the number of convolutional and fully connected layers, the size of the filters, the type of activation functions, and the type of pooling. The weights of the filters and the biases of the fully connected layers are learned from the training data using an optimization algorithm, such as stochastic gradient descent or Adam.\nA classic CNN architecture would look something like this (Figure 1):\nFigure 1: Framework of a Convolutional Neural Network ,Illustration by [Mathworks](https://uk.mathworks.com/help/deeplearning/ug/introduction-to-convolutional-neural-networks.html) For more information on CNNs, you can check out this cool blog post.\nImporting the libraries # We will be using the following libraries for this case study:\n#Importing the libraries #standard imports import numpy as np import pandas as pd import matplotlib.pyplot as plt #sklearn standard functions from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error #standard imports for pytorch import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from torch.autograd import Variable from torch.utils.data import DataLoader, TensorDataset from torch import Tensor from torch.profiler import profile, record_function, ProfilerActivity #torchvision imports import torchvision import torchvision.transforms as transforms #Other imports import time import tqdm as tqdm import seaborn as sns sns.set_theme(style=\u0026#34;whitegrid\u0026#34;) Data preparation and preprocessing # The first step in any machine learning project is to prepare the data. In this step, we will be loading the data, performing data preprocessing, and splitting the data into training and test sets.\nimport ssl ssl._create_default_https_context = ssl._create_unverified_context #Define data pre-processing steps transform = transforms.Compose( [ #Resize images for (64*64) transforms.Resize((64,64)), #Converts images into Pytorch tensor #Pytorch tensors are multi-dimensional arrays that can be processed on GPUs transforms.ToTensor(), #Normalise the input data #input data is transformed by subtracting the mean and dividing by the standard deviation for each channel. transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) #Batch size defines the number of samples processed before the model is updated. batch_size = 40 #Loading EuraSAT and transform using the defined function dataset = torchvision.datasets.EuroSAT(root=\u0026#39;./data\u0026#39;, download=True, transform=transform) #Data loader creates a PyTorch data loader for a given dataset. #The data loader provides an efficient way to iterate over the data in the dataset #and apply batch processing during training. #num_workers: defines the number of threads to use for loading the data. #If shuffle=True, the data loader will randomly shuffle the data before each epoch to ensure that the model sees a different set of samples each time it is trained. data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2) #Classes -\u0026gt; we have 10 labels #\u0026#39;AnnualCrop\u0026#39;, \u0026#39;Forest\u0026#39;, \u0026#39;HerbaceousVegetation\u0026#39;, \u0026#39;Highway\u0026#39;, \u0026#39;Industrial\u0026#39;, \u0026#39;Pasture\u0026#39;, \u0026#39;PermanentCrop\u0026#39;, \u0026#39;Residential\u0026#39;, \u0026#39;River\u0026#39; \u0026#39;SeaLake\u0026#39; classes = data_loader.dataset.classes split=len(dataset.targets)/4 train_len=int(len(dataset.targets)-split) val_len=int(split) #Spliting dataset in 75% training, 25% for testing trainset,testset = torch.utils.data.random_split(dataset, [train_len,val_len]) #Create dataloader for training and testing dataset train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2) Visualizing the data # Let\u0026rsquo;s visualize some images in the dataset.\nimport os import random from PIL import Image ROOT_dir = \u0026#39;./data/eurosat/2750\u0026#39; folders = os.listdir(ROOT_dir) plt.figure(figsize=(16,10)) for i, label in enumerate(folders): plt.subplot(4,5,i+1) file_path = os.listdir(\u0026#34;{}/{}\u0026#34;.format(ROOT_dir,label)) image_ = Image.open(ROOT_dir+\u0026#34;/\u0026#34;+label+\u0026#34;/\u0026#34;+file_path[random.randint(1,100)]) plt.imshow(image_) plt.title(label) plt.axis(\u0026#34;off\u0026#34;) Figure 2: Example classes and images Creating your CNN model for training # Now that we have prepared the data, we can create our CNN model. We will be using the following architecture for our model:\nimport torch.nn as nn import torch.nn.functional as F #Custom class extends the functionality of nn.Module class from PyTorch, #which provides the basic building blocks for creating neural networks in PyTorch. class Net(nn.Module): #Setting up layers in CNN def __init__(self): #Calling function from nn.Module super().__init__() #A 2D convolutional layer with 3 input channels, 6 output, and kernel (filter size) size of 5x5 self.conv1 = nn.Conv2d(3, 6, 5) #A max-pooling layer with kernel size 2x2 and stride of 2 self.pool = nn.MaxPool2d(2, 2) #Another convolution layer with 6 input channels, 16 output channels, and a kernel size of 5x5 self.conv2 = nn.Conv2d(6, 16, 5) #Three fully-connected linear layers for processing the output of the second convolution network self.fc1 = nn.Linear(16 * 13 * 13, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) #Define the foward pass of the network i.e. the computation performed on each input tensor. def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = torch.flatten(x, 1) # flatten all dimensions except batch x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x We can use the following code to print out the summary of the model:\nfrom torchsummary import summary summary(Net(), (3,64,64),device=\u0026#39;cpu\u0026#39;) ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 6, 60, 60] 456 MaxPool2d-2 [-1, 6, 30, 30] 0 Conv2d-3 [-1, 16, 26, 26] 2,416 MaxPool2d-4 [-1, 16, 13, 13] 0 Linear-5 [-1, 120] 324,600 Linear-6 [-1, 84] 10,164 Linear-7 [-1, 10] 850 ================================================================ Total params: 338,486 Trainable params: 338,486 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 0.31 Params size (MB): 1.29 Estimated Total Size (MB): 1.65 ---------------------------------------------------------------- Inspecting CPU/GPU usage with PyTorch Profiler and TensorBoard # Before detailing the steps to train the model, we will first look at how to use the PyTorch profiler to inspect the CPU and GPU usage of the model. PyTorch Profiler is useful for measuring the training performance and resource utilization of your model. It tracks sequences of the execution steps that are the most costly in time and memory and visualize the workload distribution between GPUs and CPUs.\nFirstly, let\u0026rsquo;s define a function to train the model. This function will be used to train the model for each batch of data. One important thing to note is that we will be using the to() function to copy the data to the device the model is on.\ndef train(model,data,criterion, optimizer,device = device): # Copy the data to the device the model is on inputs, labels = data[0].to(device=device), data[1].to(device=device) #Predict the output for given input outputs = model(inputs) #Compute the loss loss = criterion(outputs, labels) #Clear the previous gradients, compute gradients of all variables wrt loss optimizer.zero_grad() #Backpropagation, update weights loss.backward() #Update the parameters optimizer.step() Next, we can use the profiler to record the execution steps and save the logs to a file. We can then use TensorBoard to visualize the logs. The profiler includes a number of options to customize the profiling behavior. In this example, we will use the following options:\nschedule: defines the number of steps to wait before starting the profiling, the number of steps to run the profiling for, and the number of steps to repeat the profiling for. In this example, with repeat=4, profiler will record 4 spans, each span consists of 2 wait step, 2 warmup step and 3 active steps. For more information about wait/warmup/active, you can find it [here](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe html#using-profiler-to-analyze-long-running-jobs#using-profiler-to-analyze-long-running-jobs). It is important to note we are not training the whole model in this example, as it would take a long time to run. Instead, we are only training the model for a few steps. on_trace_ready: defines the action to take when the profiling is complete. In this example, we will save the profiling logs to a file that can be used by TensorBoard. profile_memory: enables memory profiling to measure tensor memory allocation/deallocation. #GPU ---------------------------- #Initialise model #Define device on cuda:0 device = torch.device(\u0026#39;cuda:0\u0026#39;) model = Net().to(device=device) #Define loss function loss_fn = nn.CrossEntropyLoss().cuda()#Loss function computes the value between the predicted values and the labels. In this case, we are using Cross-Entropy loss, but many other loss functions are also avaible from nn. Such as focal loss #Define optimizer function optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #Optimizer function aims to reduce the loss function\u0026#39;s value by changing the weight vector values through backpropagation in neural networks. We are using Stochastic gradient decent as our optimiser, with learning rate 0.01 and momentum 0.9 #Set random seed for reproducibility torch.cuda.manual_seed(42) #Profiler with torch.profiler.profile( schedule=torch.profiler.schedule( wait=2, warmup=2, active=3, repeat=4), #Saving the profiling logs to a file that can be used by TensorBoard on_trace_ready=torch.profiler.tensorboard_trace_handler(\u0026#39;./log/gpu_profile\u0026#39;), profile_memory=True, ) as prof: for step, batch_data in enumerate(train_loader,0): if step \u0026gt;= (2 + 2 + 3) * 4: break train(model =model , data =batch_data, criterion = loss_fn, optimizer = optimizer,device=device) prof.step() #CPU ---------------------------- #Reinitialise model, loss function, optimizer and random seed device = torch.device(\u0026#39;cpu\u0026#39;) model = Net().to(device=device) loss_fn = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) torch.manual_seed(42) with torch.profiler.profile( schedule=torch.profiler.schedule( wait=2, warmup=2, active=3, repeat=4), #Saving the profiling logs to a file that can be used by TensorBoard on_trace_ready=torch.profiler.tensorboard_trace_handler(\u0026#39;./log/cpu_profile\u0026#39;), profile_memory=True, ) as prof: for step, batch_data in enumerate(train_loader,0): if step \u0026gt;= (2 + 2 + 3) * 4: break train(model =model , data =batch_data, criterion = loss_fn, optimizer = optimizer,device=device) prof.step() We can then use TensorBoard to visualize the profiling logs. The following command will launch TensorBoard and open the profiling dashboard.\n%load_ext tensorboard %tensorboard --logdir ./log Or in VSCode, you can press Ctrl+Shift+P and type \u0026ldquo;Open TensorBoard\u0026rdquo;. Then select the log directory.\nThe TensorBoard profiling dashboard includes a number of tabs that can be used to visualize the profiling logs. In this example, we will be focusing on the Overview which provides a high-level overview of the profiling results. The details of these metrics are here.\nFigure 3: Baseline - GPU tensorboard Figure 4: Baseline - CPU tensorboard From Figure 3, it can be observed that the model running on CUDA is not fully utilizing the GPU, with GPU utilization at around 9% and CPU utilization at around 45%. This suggests that the GPU is not being used to its full capacity, and the CPU is carrying out most of the work. This could be due to the size of the data and the model, as the small data size and parameter values in this example can result in a significant overhead in transferring data from the CPU to the GPU. In contrast, the model running on the CPU is not invoking the GPU kernel at all, and most of the work is being carried out by the CPU itself (Figure 4). We can further investigate the profiling logs with the following command line:\nprint(prof.key_averages().table(sort_by=\u0026#34;cpu_time_total\u0026#34;, row_limit=10)) The GPU profiling logs indicate that the CUDA model is taking an average of 28.665 ms on the CPU and 4.244 ms on the GPU to run. The majority of the time is spent on the enumerate(DataLoader) and ProfilerStep function, which is the function defined to profile and loop over the data. The data transfer from the CPU to the GPU at each step creates a significant overhead, as can be observed from the aten::copy_ function, which copies the array back to CUDA as a tensor. In contrast, the CPU model takes about 35.136 ms to run - not much slower than the GPU model. However, the CPU profiling logs reveal that the most expensive function is the actual calculation steps for computing convolution and backpropagation.\n#GPU profile log ---------------------------- ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg CPU Mem Self CPU Mem CUDA Mem Self CUDA Mem # of Calls ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ProfilerStep* 38.03% 10.902ms 80.41% 23.050ms 7.683ms 0.000us 0.00% 2.117ms 705.667us 1.88 Mb -5.63 Mb 0 b -46.64 Mb 3 enumerate(DataLoader)_MultiProcessingDataLoaderIter... 13.73% 3.935ms 14.02% 4.019ms 1.340ms 0.000us 0.00% 0.000us 0.000us 7.50 Mb 7.50 Mb 0 b 0 b 3 aten::to 0.06% 17.000us 8.47% 2.428ms 142.824us 0.000us 0.00% 1.079ms 63.471us 0 b 0 b 5.63 Mb 0 b 17 cudaLaunchKernel 8.42% 2.415ms 8.42% 2.415ms 8.564us 0.000us 0.00% 0.000us 0.000us 0 b 0 b 0 b 0 b 282 aten::_to_copy 0.18% 52.000us 8.41% 2.411ms 401.833us 0.000us 0.00% 1.079ms 179.833us 0 b 0 b 5.63 Mb 0 b 6 aten::copy_ 0.27% 77.000us 7.94% 2.275ms 379.167us 1.079ms 25.42% 1.079ms 179.833us 0 b 0 b 0 b 0 b 6 cudaMemcpyAsync 6.77% 1.940ms 6.77% 1.940ms 323.333us 0.000us 0.00% 0.000us 0.000us 0 b 0 b 0 b 0 b 6 Optimizer.step#SGD.step 2.21% 633.000us 6.36% 1.822ms 607.333us 0.000us 0.00% 228.000us 76.000us -12 b -804 b 0 b 0 b 3 autograd::engine::evaluate_function: AddmmBackward0 0.78% 223.000us 5.32% 1.524ms 169.333us 0.000us 0.00% 288.000us 32.000us 0 b 0 b 5.07 Mb -1.34 Mb 9 aten::add_ 2.90% 830.000us 5.28% 1.513ms 15.760us 275.000us 6.48% 275.000us 2.865us 0 b 0 b 0 b 0 b 96 ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 28.665ms Self CUDA time total: 4.244ms #CPU profile log ---------------------------- ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg CPU Mem Self CPU Mem # of Calls ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ProfilerStep* 12.69% 4.460ms 99.79% 35.061ms 11.687ms -12 b -23.04 Mb 3 autograd::engine::evaluate_function: ConvolutionBack... 0.24% 84.000us 27.60% 9.697ms 1.616ms -14.81 Mb -17.31 Mb 6 ConvolutionBackward0 0.13% 45.000us 27.36% 9.613ms 1.602ms 2.50 Mb 0 b 6 aten::convolution_backward 26.92% 9.460ms 27.23% 9.568ms 1.595ms 2.50 Mb 0 b 6 aten::conv2d 0.10% 34.000us 15.13% 5.316ms 886.000us 14.84 Mb 0 b 6 aten::convolution 0.33% 115.000us 15.03% 5.282ms 880.333us 14.84 Mb 0 b 6 aten::_convolution 0.20% 71.000us 14.71% 5.167ms 861.167us 14.84 Mb 0 b 6 aten::mkldnn_convolution 14.31% 5.029ms 14.50% 5.096ms 849.333us 14.84 Mb 0 b 6 enumerate(DataLoader)_MultiProcessingDataLoaderIter... 11.36% 3.990ms 11.54% 4.053ms 1.351ms 5.63 Mb 5.63 Mb 3 aten::max_pool2d 1.39% 488.000us 6.47% 2.275ms 379.167us 11.13 Mb 2.47 Mb 6 ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 35.136ms How can we improve the computation speed on GPU? # Why do we care about profiling the network? Because we want to know where the bottlenecks are, and how we can improve the performance of the network. In case study 1, we saw that GPU is not always faster than CPU, and we found that the bottleneck is the data transfer between CPU and GPU. There are a few ways to improve the performance on the GPU.\nIncrease the batch size: By increasing the batch size, the number of times the data needs to be transferred between the CPU and GPU can be reduced, thus reducing the overhead.\nUse larger models and datasets: Larger models and datasets would increase the time taken for calculations on the CPU, making the overhead of transferring data from CPU to GPU a smaller proportion of the total time.\nUse data prefetching: By loading the next batch of data onto the GPU while the current batch is being processed, the overhead of data transfer can be reduced.\nUse different optimizers: The choice of optimizer may also affect the performance of the model depending on the complexity of the model, the amount of training data, and the available computational resources. For example, you might use the stochastic gradient descent (SGD) optimizer for a simple linear model, while you might use more advanced optimizers like Adam or Adagrad for more complex deep learning models.\nCase Study 2: Comparing model performance with a fine-tune model # In this case study, we will be continuing our experiments with a pre-trained model. We will be using the same model and dataset as in case study 1. The only difference is that we will be a model that has already been trained on a large dataset.\nPre-training and fine-tuning # Pretraining and fine-tuning are popular techniques in computer vision for improving the accuracy of deep learning models. Pretraining refers to training a deep learning model on a large dataset, typically using a general task like image classification or object detection, to learn general features that can be useful for a variety of tasks. These pretrained models are then fine-tuned on a smaller dataset specific to the target task. Fine-tuning involves taking the pretrained model and updating the weights of the final few layers to optimize for the target task. This allows the model to learn task-specific features and adapt to the new dataset, while still leveraging the general features learned during pretraining. By using pretrained models and fine-tuning, deep learning models can achieve higher accuracy on smaller datasets, reduce the amount of data required for training, and speed up the training process.\nOne example of a pretrained model is the VGG16 model, which was trained on the ImageNet dataset. The ImageNet dataset contains over 14 million images and 1000 classes. The VGG16 model was trained on the ImageNet dataset to learn general features that can be useful for a variety of tasks. The VGG16 model can be downloaded from the torchvision library.\nInitializing the model # We will be using the VGG16 model as our pretrained model. The VGG16 model has 16 layers, including 13 convolutional layers and 3 fully connected layers. The model is pretrained on the ImageNet dataset, which contains 1000 classes. We will be using the VGG16 model to classify our satellite imageries. To begin, we need to download the pretrained network and change the final fully connected layer of the VGG16 model to output 10 classes instead of 1000 classes.\nvgg11_bn = torchvision.models.vgg11_bn(weights=True) # Freeze weights of all layers except the new classification layer for param in vgg11_bn.parameters(): param.requires_grad = False num_ftrs = vgg11_bn.classifier[6].in_features # Replace the final classfication layer vgg11_bn.classifier[6] = nn.Linear(num_ftrs,len(classes)) vgg11_bn.classifier[6].requires_grad = True vgg11_bn.train() Quick inspection of the model # Again we can use the summary function to see the architecture of the model.\nThe vgg16 model has in total 128,812,810 parameters, and estimated total size of 506.64 Mb, both of which are much larger than the model we trained in case study 1.\nsummary(vgg11_bn, (3,64,64),device=\u0026#39;cpu\u0026#39;) ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 64] 1,792 BatchNorm2d-2 [-1, 64, 64, 64] 128 ReLU-3 [-1, 64, 64, 64] 0 MaxPool2d-4 [-1, 64, 32, 32] 0 Conv2d-5 [-1, 128, 32, 32] 73,856 BatchNorm2d-6 [-1, 128, 32, 32] 256 ReLU-7 [-1, 128, 32, 32] 0 MaxPool2d-8 [-1, 128, 16, 16] 0 Conv2d-9 [-1, 256, 16, 16] 295,168 BatchNorm2d-10 [-1, 256, 16, 16] 512 ReLU-11 [-1, 256, 16, 16] 0 Conv2d-12 [-1, 256, 16, 16] 590,080 BatchNorm2d-13 [-1, 256, 16, 16] 512 ReLU-14 [-1, 256, 16, 16] 0 MaxPool2d-15 [-1, 256, 8, 8] 0 Conv2d-16 [-1, 512, 8, 8] 1,180,160 BatchNorm2d-17 [-1, 512, 8, 8] 1,024 ReLU-18 [-1, 512, 8, 8] 0 Conv2d-19 [-1, 512, 8, 8] 2,359,808 BatchNorm2d-20 [-1, 512, 8, 8] 1,024 ReLU-21 [-1, 512, 8, 8] 0 MaxPool2d-22 [-1, 512, 4, 4] 0 Conv2d-23 [-1, 512, 4, 4] 2,359,808 BatchNorm2d-24 [-1, 512, 4, 4] 1,024 ReLU-25 [-1, 512, 4, 4] 0 Conv2d-26 [-1, 512, 4, 4] 2,359,808 BatchNorm2d-27 [-1, 512, 4, 4] 1,024 ReLU-28 [-1, 512, 4, 4] 0 MaxPool2d-29 [-1, 512, 2, 2] 0 AdaptiveAvgPool2d-30 [-1, 512, 7, 7] 0 Linear-31 [-1, 4096] 102,764,544 ReLU-32 [-1, 4096] 0 Dropout-33 [-1, 4096] 0 Linear-34 [-1, 4096] 16,781,312 ReLU-35 [-1, 4096] 0 Dropout-36 [-1, 4096] 0 Linear-37 [-1, 10] 40,970 ================================================================ Total params: 128,812,810 Trainable params: 40,970 Non-trainable params: 128,771,840 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 15.21 Params size (MB): 491.38 Estimated Total Size (MB): 506.64 ---------------------------------------------------------------- Inspecting CPU and GPU usage with VGG16 # Again, we can repeat the same process as in case study 1 to profile CPU and GPU usage - we can change the model input to VGG16 using vgg11_bn = VGG.to(device=device).\nFigure 5: VGG16 - GPU tensorboard Figure 6: VGG16 - CPU tensorboard Figure 5 and 6 show the GPU and CPU usage for VGG16. We can see that the GPU Utilization and memory usage are much higher than our baseline model in case study 1. This is because the required calculations are much higher for the VGG16 model. Therefore, in theory, we should expect the GPU runtime on VGG16 should outperform the CPU runtime significantly.\nTraining the model # def train_model(model, dataloaders, criterion, optimizer, num_epochs=10, device=device): # Initialize time since = time.time() # Initialize reporting metrics train_acc_history,val_acc_history,train_loss_history,val_loss_history = [],[],[],[] best_acc = 0.0 for epoch in range(num_epochs): print(\u0026#39;Epoch {}/{}\u0026#39;.format(epoch, num_epochs - 1)) print(\u0026#39;-\u0026#39; * 10) # Each epoch has a training and validation phase for phase in [\u0026#39;train\u0026#39;, \u0026#39;val\u0026#39;]: if phase == \u0026#39;train\u0026#39;: model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == \u0026#39;train\u0026#39;): # Get model outputs and calculate loss outputs = model(inputs) loss = criterion(outputs, labels) _, preds = torch.max(outputs, 1) # backward + optimize only if in training phase if phase == \u0026#39;train\u0026#39;: loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data).type(torch.float).item() epoch_loss = running_loss / len(dataloaders[phase].dataset) epoch_acc = running_corrects / len(dataloaders[phase].dataset) print(\u0026#39;{} Loss: {:.4f} Acc: {:.4f}\u0026#39;.format(phase, epoch_loss, epoch_acc)) if phase == \u0026#39;train\u0026#39;: train_acc_history.append(epoch_acc) train_loss_history.append(epoch_loss) else: val_acc_history.append(epoch_acc) val_loss_history.append(epoch_loss) print() time_elapsed = time.time() - since print(\u0026#39;Training complete in {:.0f}m {:.0f}s\u0026#39;.format(time_elapsed // 60, time_elapsed % 60)) return train_acc_history,val_acc_history,train_loss_history,val_loss_history,time_elapsed Speed comparison # We can now compare the speed and accuracy of the CPU and GPU implementations of VGG16 and the baseline model. It is important to note that we are re-initializing the loss function and optimizer on the same device as the model. This is because the loss function and optimizer are also part of the model and will be moved to the same device as the model.\n#VGG16 #CPU---------------------------------------------------------------------------------------------------------------- device = torch.device(\u0026#39;cpu\u0026#39;) vgg11_cpu = vgg11_bn.to(device=device) loss_fn = nn.CrossEntropyLoss() optimizer = optim.SGD(vgg11_cpu.parameters(), lr=0.01, momentum=0.9) train_acc_history,val_acc_history,train_loss_history,val_loss_history,vgg_cpu_time = train_model(vgg11_cpu,dataloader_all,loss_fn, optimizer, num_epochs=10, device=device) #GPU---------------------------------------------------------------------------------------------------------------- device = torch.device(\u0026#39;cuda:0\u0026#39;) vgg11_gpu = vgg11_bn.to(device=device) loss_fn = nn.CrossEntropyLoss().cuda() optimizer = optim.SGD(vgg11_gpu.parameters(), lr=0.01, momentum=0.9) train_acc_history,val_acc_history,train_loss_history,val_loss_history,vgg_gpu_time = train_model(vgg11_gpu,dataloader_all,loss_fn, optimizer, num_epochs=10, device=device) #Baseline #CPU---------------------------------------------------------------------------------------------------------------- device = torch.device(\u0026#39;cpu\u0026#39;) base_cpu = Net().to(device=device) loss_fn = nn.CrossEntropyLoss() optimizer = optim.SGD(base_cpu.parameters(), lr=0.01, momentum=0.9) base_train_acc_history,base_val_acc_history,base_train_loss_history,base_val_loss_history,baseline_cpu_time = train_model(base_cpu,dataloader_all,loss_fn, optimizer, num_epochs=10, device=device) #GPU---------------------------------------------------------------------------------------------------------------- device = torch.device(\u0026#39;cuda:0\u0026#39;) base_gpu = Net().to(device=device) loss_fn = nn.CrossEntropyLoss().cuda() optimizer = optim.SGD(base_gpu.parameters(), lr=0.01, momentum=0.9) base_train_acc_history,base_val_acc_history,base_train_loss_history,base_val_loss_history,baseline_gpu_time = train_model(base_gpu,dataloader_all,loss_fn, optimizer, num_epochs=10, device=device) compute_time = pd.DataFrame([baseline_cpu_time,baseline_gpu_time,vgg_cpu_time,vgg_gpu_time],columns=[\u0026#39;Time\u0026#39;]) compute_time[\u0026#39;Model\u0026#39;] = [\u0026#39;baseline\u0026#39;,\u0026#39;baseline\u0026#39;,\u0026#39;VGG16\u0026#39;,\u0026#39;VGG16\u0026#39;] compute_time[\u0026#39;Mode\u0026#39;] = [\u0026#39;CPU\u0026#39;,\u0026#39;GPU\u0026#39;,\u0026#39;CPU\u0026#39;,\u0026#39;GPU\u0026#39;] g= sns.catplot(data=compute_time, kind=\u0026#39;bar\u0026#39;,x=\u0026#39;Model\u0026#39;,y=\u0026#39;Time\u0026#39;,hue=\u0026#39;Mode\u0026#39;) g.set_axis_labels(\u0026#34;\u0026#34;, \u0026#34;Total Time (Seconds)\u0026#34;) Figure 7: Speed comparisons between GPU and CPU on Baseline and VGG16 Based on Figure 7, it is evident that the VGG16 model implemented on a GPU is approximately 6 times faster than the same model implemented on a CPU. This can be attributed to the fact that the GPU is capable of parallel processing, thereby reducing the total time required for training the model. However, in the case of the baseline model, the GPU implementation does not exhibit a significant improvement over the CPU implementation. This is due to the fact that the baseline model is relatively simple and does not involve a large amount of computation.\nComparison of Model Accuracy # The final step is to evaluate the training model on the testing dataset. There are many ways to evaluate the performance of a model. In this case, we will use the accuracy score, which is defined as the number of correct predictions divided by the total number of predictions, and the confusion matrix, which is a table that shows the number of correct and incorrect predictions for each class. The following code block shows how to calculate the accuracy score and confusion matrix for the VGG16 and baseline model.\nfig,ax = plt.subplots(2,1,figsize=(20,20)) def show_heatmap(test_loader,model,ax,name): heatmap = pd.DataFrame(data=0,index=classes,columns=classes) with torch.no_grad(): number_corrects = 0 number_samples = 0 for images, labels in test_loader: images, labels = images.to(device), labels.to(device) outputs = model(images) _, predicted = torch.max(outputs, 1) number_corrects += (predicted==labels).sum().item() number_samples += labels.size(0) for i in range(len(labels)): true_label = labels[i].item() predicted_label = predicted[i].item() heatmap.iloc[true_label,predicted_label] += 1 sns.heatmap(heatmap, annot=True, fmt=\u0026#34;d\u0026#34;,cmap=\u0026#34;YlGnBu\u0026#34;,ax=ax) ax.set_title(f\u0026#39;{name}, Overall accuracy {(number_corrects / number_samples)*100}%\u0026#39;) show_heatmap(test_loader,vgg11_gpu,ax[0],\u0026#34;VGG16\u0026#34;) show_heatmap(test_loader,base_gpu,ax[1],\u0026#34;Baseline\u0026#34;) Figure 8: Overall accuracy and confusion matrix From Figure 8, it is evident that the VGG16 model has a higher overall accuracy than the baseline model. This is expected because the VGG16 model is a more complex model that is capable of learning more complex patterns in the data. The baseline model is still able to achieve a respectable accuracy of 71% - in future experiments, we can try to improve the accuracy of the baseline model by increasing the number of epochs, optimizing the learning rate, or adding more layers to the model.\nConclusion # In this case study, we showcased the utilization of GPUs to accelerate the classification of satellite images, alongside the use of profiling tools like Tensorboard to visualize CPU/GPU bottlenecks. Our findings show that deep learning models can benefit greatly from GPUs, especially when the model is complex and the data size is substantial.\nWhile deep learning frameworks like PyTorch and TensorFlow offer a simple API to transfer model/tensor objects from the CPU to the GPU, the overhead of transferring data from the CPU to the GPU can be significant for smaller data or model sizes. Therefore, the performance of the GPU implementation may only be marginally better than that of the CPU implementation in such cases. Consequently, it is imperative to evaluate the data and computation sizes thoroughly before deciding to use a CPU or a GPU.\n"},{"id":2,"href":"/GPU-Analytics/docs/case_study/chapter_3/","title":"Chapter 3 - Geospatial Operation and Analysis","section":"Case Studies","content":" Introduction # Why are we interested in geospatial data? Geospatial data is a type of data that is associated with a location. This location can be a point, a line, a polygon, or a raster. Geospatial data is becoming more and more important, yet the sheer volume of information that is generated made it difficult to handle. In this chapter, we will be exploring the use of GPU for manipulating large-scale geospatial data, and provide a practical example of how we can predict the presence of Aedes aegypti across the globe.\nObjectives # The objective of the third Case Study is to demonstrate the practical application of the common spatial data structures and operation with GPU-accelerated Python packages. The goal here is in twofold: 1) compare the computational speed of calculating point in polygon and 2) compare the computational speed of a classification model with raster data.\nPredicting the Presence of Aedes aegypti # In this case study, we will be predicting the global presence and probability of Aedes aegypti, a mosquito species that is known to transmit dengue fever, chikungunya, and Zika virus. You can download the Aedes aegypti point occurrence data across the World from 1958 to 2014 here. We will also be using precipitation, temperature, elevation, and population density as predictor variables to capture the climatic, environmental and demographics variables. You can download the raster data from the GitHub repository here.\nLoad datasets # To begin, let\u0026rsquo;s load the necessary libraries and datasets. It is important to note that all shape file and raster data (5km resolution) were projected to the CRS: WGS84 4236.\n#Import libraries import rasterio import geopandas as gpd import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import cuspatial import cudf # Load raster data precipitation = rasterio.open(\u0026#39;./data/Global Raster/Precipitation/Global Averaged Precipitation 2000-14.tif\u0026#39;) temp = rasterio.open(\u0026#39;./data/Global Raster/Temperature/Global Averaged Temperature 2000-14.tif\u0026#39;) elevation = rasterio.open(\u0026#39;./data/Global Raster/Elevation/Global Land Surface Elevation.tif\u0026#39;) pop_density = rasterio.open(\u0026#39;./data/Global Raster/Population Density/Global Population Density AveragedEst.tif\u0026#39;) # Load Shapefile #Loading global shapefile global_outline =gpd.read_file(\u0026#39;./data/Global Shapefile/Global Outline/Global_All_0.shp\u0026#39;,crs=\u0026#39;EPSG:4326\u0026#39;) #Loading country shapefiles country_outline = gpd.read_file(\u0026#39;./data/Global Shapefile/Global with Country Outline/Global_Countries_1.shp\u0026#39;,crs=\u0026#39;EPSG:4326\u0026#39;) # Load Point occurrence data aedes =pd.read_csv(\u0026#39;./data/Global Raster/aedes_point.csv\u0026#39;) aedes_point = gpd.GeoDataFrame(aedes,geometry=gpd.points_from_xy(aedes[\u0026#39;longitude\u0026#39;],aedes[\u0026#39;latitude\u0026#39;]),crs=\u0026#39;EPSG:4326\u0026#39;) # Transformation global_outline.crs = (\u0026#39;+init=EPSG:4326\u0026#39;) country_outline.crs =(\u0026#39;+init=EPSG:4326\u0026#39;) aedes_point.crs = (\u0026#39;+init=EPSG:4326\u0026#39;) # Check CRS print(global_outline.crs ==aedes_point.crs) Data Visualization # Let\u0026rsquo;s visualize the point occurrence data and the predictor variables. We will be using the matplotlib and rasterio packages to visualize the data.\n# Visualise point patterns import matplotlib.pyplot as plt fig, ax = plt.subplots(1,1, figsize = (10,10)) country_outline.plot(edgecolor=\u0026#39;black\u0026#39;,linewidth=0.1,ax=ax,color=\u0026#34;white\u0026#34;) aedes_point.plot(ax=ax,color=\u0026#39;red\u0026#39;,markersize = 1) #show(light,ax=ax,cmap=\u0026#39;viridis\u0026#39;) ax.axis(\u0026#39;off\u0026#39;) ax.set_title(\u0026#34;Distribution of aedes occurence across the World\u0026#34;) Figure 1: Global Distribution of Aedes aegypti from rasterio.plot import show fig, ax = plt.subplots(nrows=2,ncols=2, figsize=(20,20)) title =[\u0026#34;precipitation\u0026#34;,\u0026#34;temp\u0026#34;,\u0026#34;elevation\u0026#34;,\u0026#34;pop_density\u0026#34;] for index,attribute in enumerate([precipitation,temp,elevation,pop_density]): image = show(attribute,ax=ax[index//2,index%2],cmap=\u0026#39;nipy_spectral\u0026#39;,title = title[index]) image.axis(\u0026#39;off\u0026#39;) fig.subplots_adjust(hspace=-0.5, wspace=0) Figure 2: Distribution of Predictors: Precipitation, Temperature, Elevation, and Population Density Prepare data for pseudo-background points as absence # Next, we need to prepare the background data. What is the background data? With Background data we are not attempting to guess point locations where an event is absent. Here, we are rather trying to characterise the environment of the study region. In this sense, background is the same, irrespective of where the point fire are found or not. Background data establishes the environmental domain of the study, whilst presence data should establish under which conditions a fire is more likely to be present than on average.\nThere are several ways to generate background data. In R, we can use the spsample() function from the sp package to generate randomly-distributed points with defined spatial boundaries. However, in Python, there is no pre-built function to help us. Instead, we will generate random points across the world and filter out the points that are outside of the country boundary. Here is the code to generate random points across the world.\n# random seed import random from shapely.geometry import Point from tqdm import tqdm random.seed(10) def Random_Points_in_Polygon(polygon, number): bound = polygon.bounds.values[0] minx, miny, maxx, maxy = bound[0],bound[1],bound[2],bound[3] x_point,y_point = np.random.uniform(minx, maxx,size=number),np.random.uniform(miny,maxy,size=number) return x_point,y_point background_points = Random_Points_in_Polygon(global_outline,aedes_point.shape[0]*5) background_points_shp = gpd.GeoDataFrame(geometry=gpd.points_from_xy(x=background_points[0],y=background_points[1]),crs=\u0026#39;EPSG:4326\u0026#39;) background_points_shp.crs = global_outline.crs print(\u0026#34;Number of background points: \u0026#34;,background_points_shp.shape[0]) #Number of background points: 171108 Figure 3 shows the distribution of background points across the world. We generated 171,108 points that are evenly distributed across the world, with some points located in the ocean.\n# Visualise point patterns fig, ax = plt.subplots(1,1, figsize = (10,10)) global_outline.plot(edgecolor=\u0026#39;black\u0026#39;,linewidth=2,ax=ax,color=\u0026#34;white\u0026#34;) background_points_shp.plot(ax=ax,color=\u0026#39;blue\u0026#39;,markersize = 0.1) ax.axis(\u0026#39;off\u0026#39;) ax.set_title(\u0026#34;Distribution of background points across the World\u0026#34;) Figure 3: Generating pseudo-background points as absence Calculate point in polygon with GeoPandas and CuSpatial # Now that we\u0026rsquo;ve created the background points, we need to quickly determine which points occur in each country. This task is commonly known as a Point in Polygon (PIP) query, however, they are usually extremely slow. In Python, we can use the Shapely library\u0026rsquo;s functions .within() and .contains() to determine if a point is within a polygon or if a polygon contains a point. Nevertheless, these functions are not designed to handle large datasets. Assuming that we have 1 million points and 100 polygons, the .within() function will take 100 million comparisons to determine which points are within which polygons. This is a very slow process.\nLet\u0026rsquo;s first use the .sjoin() function from GeoPandas to determine which points are within which polygons. Using the %%time magic function, we can see that the .sjoin() function takes 8.6 seconds to compute 171108 points.\n# pip query with geopandas %%time pointInPolys = gpd.sjoin(background_points_shp, country_outline) #CPU times: user 8.6 s, sys: 43 ms, total: 8.65 s #Wall time: 8.69 s Next, we can use the .point_in_polygon function from the GPU-accelerated library cuSpatial. It is important to note here that the .point_in_polygon requires the points to be in the form of a cudf.DataFrame and the polygons to be in the form of a cuspatial.GeoSeries. Here, we are using the cuspatial.read_polygon_shapefile() function to make sure the resulting tuple poly_offsets, poly_ring_offsets, poly_points perfectly matches the input requirements of point_in_polygon.\nBecause the .point_in_polygon function can only handle 31 polygons at a time, we need to split the polygons into batches of 31 polygons. The for loop shown below iterates through each batch and append true values in the array to a new Country ID, matching the spatial indices of the polygons.\nThe .point_in_polygon function performs the PIP query in 0.5 seconds, which is 16 times faster than the .sjoin() function.\n%%time # Cuspatial cu_countries_outline = cuspatial.read_polygon_shapefile(\u0026#39;./data/Global Shapefile/Global with Country Outline/Global_Countries_1.shp\u0026#39;) background_points_df = cudf.DataFrame() background_points_df[\u0026#39;Long\u0026#39;] = background_points[0] background_points_df[\u0026#39;Lat\u0026#39;] = background_points[1] background_points_df[\u0026#39;LocationID\u0026#39;] = cu_countries_outline[0].shape[0] pip_iterations = list(np.arange(0, cu_countries_outline[0].shape[0], 31)) for i in range(len(pip_iterations)-1): start = pip_iterations[i] end = pip_iterations[i+1] pip_countries = cuspatial.point_in_polygon(background_points_df[\u0026#39;Long\u0026#39;],background_points_df[\u0026#39;Lat\u0026#39;],cu_countries_outline[0][start:end],cu_countries_outline[1],cu_countries_outline[2][\u0026#39;x\u0026#39;],cu_countries_outline[2][\u0026#39;y\u0026#39;]) for j in pip_countries.columns: background_points_df[\u0026#39;LocationID\u0026#39;].loc[pip_countries[j]] = j #CPU times: user 553 ms, sys: 8.88 ms, total: 562 ms #Wall time: 563 ms Stress Test # The differences may be marginal at this point, but the .point_in_polygon function will become more significant as the number of background points increases. We can run a stress test by increasing the number of background points. The code below generates 10-30 times more background points than the original dataset. We can see from Figure 4 that the .sjoin() function takes linearly longer to compute the PIP query, while the .point_in_polygon function takes a constant time to compute the PIP query, with average runtimes of less than 2.5 seconds.\nimport time cpu_time = [] gpu_time = [] for i in range(10,30): #Generate random background points background_points = Random_Points_in_Polygon(global_outline, aedes_point.shape[0] * i) #Preparing data on Geopandas background_points_shp = gpd.GeoDataFrame(geometry=gpd.points_from_xy(x=background_points[0], y=background_points[1]), crs=\u0026#39;EPSG:4326\u0026#39;) background_points_shp.crs = global_outline.crs #cpu runtime -------------------------------- start = time.time() CPU_pointInPolys = gpd.sjoin(background_points_shp, country_outline) end = time.time() cpu_time.append(end - start) #Preparing data on cudf background_points_df = cudf.DataFrame() background_points_df[\u0026#39;Long\u0026#39;] = background_points[0] background_points_df[\u0026#39;Lat\u0026#39;] = background_points[1] background_points_df[\u0026#39;LocationID\u0026#39;] = cu_countries_outline[0].shape[0] pip_iterations = list(np.arange(0, cu_countries_outline[0].shape[0], 31)) #gpu runtime -------------------------------- start = time.time() for iter in range(len(pip_iterations) - 1): pip_start = pip_iterations[iter] pip_end = pip_iterations[iter + 1] pip_countries = cuspatial.point_in_polygon(background_points_df[\u0026#39;Long\u0026#39;], background_points_df[\u0026#39;Lat\u0026#39;], cu_countries_outline[0][pip_start:pip_end], cu_countries_outline[1], cu_countries_outline[2][\u0026#39;x\u0026#39;], cu_countries_outline[2][\u0026#39;y\u0026#39;]) for j in pip_countries.columns: background_points_df[\u0026#39;LocationID\u0026#39;].loc[pip_countries[j]] = j end = time.time() gpu_time.append(end - start) #Create a dataframe to store the results gpu_elapsed = pd.DataFrame({\u0026#34;time\u0026#34;:gpu_time,\u0026#34;data_size\u0026#34;:[aedes_point.shape[0]* i for i in range(10,30)],\u0026#39;label\u0026#39;:\u0026#34;cuspatial.point_in_polygon\u0026#34;}) cpu_elapsed = pd.DataFrame({\u0026#34;time\u0026#34;:cpu_time,\u0026#34;data_size\u0026#34;:[aedes_point.shape[0]* i for i in range(10,30)],\u0026#39;label\u0026#39;:\u0026#34;gpd.sjoin\u0026#34;}) result = pd.concat([gpu_elapsed,cpu_elapsed]).reset_index() #Plot results fig, ax = plt.subplots(figsize=(10,10)) sns.lineplot(x= \u0026#39;data_size\u0026#39;,y=\u0026#39;time\u0026#39;,hue = \u0026#39;label\u0026#39;,data = result,ax = ax ) plt.xlabel(\u0026#39;Data Size\u0026#39;) plt.ylabel(\u0026#34;Time Elapsed \u0026#34;) plt.title(\u0026#34;Comparing the speed of Point-in-Polygons calculation on CPU and GPU\u0026#34;) plt.show() Figure 4: Comparing the speed of Point-in-Polygons calculation on CPU and GPU Visualize point patterns # Now that we know how to calculate the PIP query, we can visualize the point patterns by selecting the background points that are within the polygon of a country. The code below visualizes the point patterns of the countries with the highest number of cases.\n# Visualise point patterns #Recalculate the PIP query with Cuspatial background_points = Random_Points_in_Polygon(global_outline,aedes_point.shape[0]*5) background_points_df = cudf.DataFrame() background_points_df[\u0026#39;Long\u0026#39;] = background_points[0] background_points_df[\u0026#39;Lat\u0026#39;] = background_points[1] background_points_df[\u0026#39;LocationID\u0026#39;] = cu_countries_outline[0].shape[0] pip_iterations = list(np.arange(0, cu_countries_outline[0].shape[0], 31)) for i in range(len(pip_iterations)-1): start = pip_iterations[i] end = pip_iterations[i+1] pip_countries = cuspatial.point_in_polygon(background_points_df[\u0026#39;Long\u0026#39;],background_points_df[\u0026#39;Lat\u0026#39;],cu_countries_outline[0][start:end],cu_countries_outline[1],cu_countries_outline[2][\u0026#39;x\u0026#39;],cu_countries_outline[2][\u0026#39;y\u0026#39;]) for j in pip_countries.columns: background_points_df[\u0026#39;LocationID\u0026#39;].loc[pip_countries[j]] = j pointInPolys = background_points_df.query(\u0026#34;LocationID != 250\u0026#34;) pointInPolys = gpd.GeoDataFrame(geometry = gpd.points_from_xy(pointInPolys[\u0026#39;Long\u0026#39;].to_numpy(),pointInPolys[\u0026#39;Lat\u0026#39;].to_numpy()),crs=\u0026#34;EPSG:4326\u0026#34;) import matplotlib.pyplot as plt fig, ax = plt.subplots(1,1, figsize = (10,10)) global_outline.plot(edgecolor=\u0026#39;black\u0026#39;,linewidth=1,ax=ax,color=\u0026#34;white\u0026#34;) pointInPolys.plot(ax=ax,color=\u0026#39;blue\u0026#39;,markersize = 0.1) ax.axis(\u0026#39;off\u0026#39;) ax.set_title(\u0026#34;Distribution of background points across Brazil\u0026#34;) Figure 5: Filtering the pseudo-background points that are within the country boundary ``` Create a multi-band raster from the predictor variables # To facilitate the required analysis, it is necessary to construct a multi-band raster object from the predictor variables. A band is essentially a matrix of cell values, and a raster with multiple bands comprises multiple matrices of cell values that overlap spatially and represent the same geographic region. For example, the raster object for temperature is a single-band raster object. However, if we stack raster objects for variables such as precipitation, population density and elevation on top of the temperature raster, we create a multi-band raster object. This object will have four bands, each corresponding to a single matrix of cell values.\nThe creation of this multi-band raster object is essential to perform the extraction of raster values from all variables at occurrence points in a single step. Additionally, the entire multi-band raster object is required for estimating and predicting spatial data.\nmeta = precipitation.meta meta.update(count = 4) with rasterio.open(\u0026#39;./data/Global Raster/global_stack.tif\u0026#39;, \u0026#39;w\u0026#39;,**meta) as dst: for index,attribute in enumerate([precipitation,temp,elevation,pop_density],start=1): dst.write(attribute.read(1),index) stack = rasterio.open(\u0026#39;./data/Global Raster/global_stack.tif\u0026#39;) Extraction of all raster values from predictor variables onto presence-absence points # Now, we are going to extract information from our raster stack to both the presence and background points. This can be done using the sample function in rasterio. For all occurrence points (i.e., presence), we need to add an indicator of 1 to signify presence; while for all background points (i.e., absence) - we need to also add an indicator of 0 to signify absence. We do this because we are modelling a probability and such niche models take outcomes that are from a Bernoulli or Binomial distribution.\n# Extrat raster values background_list = [(x,y) for x,y in zip(pointInPolys[\u0026#39;geometry\u0026#39;].x , pointInPolys[\u0026#39;geometry\u0026#39;].y)] pointInPolys[\u0026#39;value\u0026#39;] = [x for x in stack.sample(background_list)] aedes_list = [(x,y) for x,y in zip(aedes_point[\u0026#39;geometry\u0026#39;].x , aedes_point[\u0026#39;geometry\u0026#39;].y)] aedes_point[\u0026#39;value\u0026#39;] = [x for x in stack.sample(aedes_list)] # Convert into dataframe aedes_env = pd.DataFrame(np.vstack(aedes_point[\u0026#39;value\u0026#39;]),columns=title) aedes_env[\u0026#39;Presence\u0026#39;] = 1 background_env = pd.DataFrame(np.vstack(pointInPolys[\u0026#39;value\u0026#39;]),columns=title) background_env[\u0026#39;Presence\u0026#39;] = 0 #Merge input_data = pd.concat([aedes_env,background_env],axis=0) input_data[input_data\u0026lt;0] =0 #Save\nPreparation of training \u0026amp; test data for prediction \u0026amp; model cross-validation # Now, we need to prepare our data for model cross-validation. We are going to split our data into training and test data. The training data will be used to train the model, while the test data will be used to validate the model. The test data will be used to assess the model’s performance on data that it has not seen before. We are going to use a 80:20 split for training and test data, respectively.\n#Split train,test set from sklearn.model_selection import train_test_split y= input_data.pop(\u0026#39;Presence\u0026#39;) X = input_data X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8) Random Forest Classification Model with CuML and Scikit-learn # Now, we can fit the random forest model, which tries to find the combination of environmental risk factors that best predicts the occurrence of the aedes aegypti.\nA random forest is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. For more information on random forest, check here.\nFor demonstration purposes, we are going to use the random forest model from both Scikit-learn and cuML. cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions on GPU. cuML is designed to be a drop-in replacement for Scikit-learn. For more information on cuML, check here.\nAs we can see the code below, the cuML random forest model is very similar to the Scikit-learn random forest model. The only difference is that we need to convert the training and test data into CuPy arrays.\nOverall, we can see that the cuML random forest model is much faster than the Scikit-learn random forest model, with a speedup of from 4.65s to 0.34s.\n%%time # Random forest with sklearn from sklearn.ensemble import RandomForestClassifier sk_model = RandomForestClassifier(max_depth=20, random_state=42,n_estimators=100) sk_model.fit(X_train,y_train) y_pred =sk_model.predict(X_test) #CPU times: user 4.63 s, sys: 9.95 ms, total: 4.64 s #Wall time: 4.65 s %%time # Random forest with cuml import cupy as cp from cuml.ensemble import RandomForestClassifier as cuRFC cuml_model = cuRFC(max_depth=20, random_state=42,n_estimators=100) cuml_model.fit(cp.array(X_train),cp.array(y_train)) cuml_predict = cuml_model.predict(cp.array(X_test)) #CPU times: user 2.81 s, sys: 359 ms, total: 3.17 s #Wall time: 341 ms Model validation and examination of the predictor’s contribution # Now that we have fitted the random forest model, we can examine the model’s performance. We can do this by the model’s accuracy, the area under the curve (AUC) and max TPR+TNR. The AUC is a measure of the model’s performance. The higher the AUC, the better the model is at distinguishing between the presence and absence of the aedes aegypti. The max TPR+TNR denotes the probability threshold at which our model maximizes the True Positive Rate and the True Negative Rate. It is generally accepted that this is an optimum value at which to set the threshold for binary classification of the predicted probabilities in our mapping outputs. Anything above value is deemed as a region environmentally suitable for outcome.\n# Model validation from sklearn import metrics print(\u0026#34;Accuracy:\u0026#34;,metrics.accuracy_score(y_test,cp.asnumpy(cuml_predict))) #Accuracy: 0.9372446306966998 from sklearn.metrics import RocCurveDisplay y_pred_proba = cuml_model.predict_proba(X_test)[::,1] fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba) auc = metrics.roc_auc_score(y_test, y_pred_proba) plt.plot(fpr,tpr) plt.legend(loc=4) plt.plot([0, 1], [0, 1], \u0026#34;k--\u0026#34;, label=\u0026#34;chance level (AUC = 0.5)\u0026#34;) plt.axis(\u0026#34;square\u0026#34;) plt.xlabel(\u0026#34;False Positive Rate\u0026#34;) plt.ylabel(\u0026#34;True Positive Rate\u0026#34;) plt.title(\u0026#34;AUC = {}\u0026#34;.format(auc)) plt.legend() plt.show() optimal_idx = np.argmax(tpr - fpr) optimal_threshold = thresholds[optimal_idx] print(\u0026#34;Max TRP+TNR:\u0026#34;, optimal_threshold) #Max TRP+TNR: 0.36108708 From Figure 6, we can see that the AUC of the random forest model is amost 0.99, which is substantially higher than the baseline of 0.5. This means that the random forest model is able to distinguish between the presence and absence of Aedes aegypti with a high degree of accuracy. The optimal probability threshold at which our model maximizes the True Positive Rate and the True Negative Rate is 0.3611 (36.1%). Hence, we will use predicted probability \u0026gt; 0.3611 to delineate areas of suitability (or trigger points) for the presence of Aedes aegypti.\nFigure 6: ROC Curve of the Random Forest Classification Model We can also examine the feature importance of the random forest model. The feature importance is a measure of how much each predictor contributes to the model’s performance. The higher the feature importance, the more important the predictor is in predicting the outcome.\n# Feature importance import pandas as pd feature_imp = pd.Series(sk_model.feature_importances_,index=title).sort_values(ascending=False) import matplotlib.pyplot as plt import seaborn as sns # Creating a bar plot sns.barplot(x=feature_imp, y=feature_imp.index) # Add labels to your graph plt.xlabel(\u0026#39;Feature Importance Score\u0026#39;) plt.ylabel(\u0026#39;Features\u0026#39;) plt.title(\u0026#34;Visualizing Important Features\u0026#34;) plt.legend() plt.show() From Figure 7, we can see that population density is the most important predictor in predicting the presence of Aedes aegypti. However, it is important to note that cuML does not support the feature importance function.\nFigure 7: Feature importance of the Random Forest Classification Model Mapping the predicted probability of presence of Aedes aegypti # To map the predicted probability of presence of Aedes aegypti, we need to predict the probability of presence of Aedes aegypti for each grid cell. To do this, we need to convert the environmental matrices into a 2D array. We can then use the predict_proba function from sklearn and cuML to predict the probability of presence of Aedes aegypti for each grid cell.\nAgain, we can see a significant improvement in the speed of the prediction using cuML compared to sklearn, reducing the prediction time from 2min 51s to 2.06s.\n# Mapping predicted probability and suitability input_matrix = stack.read() #input shape (4, 4320, 8640) #Convert into (4, 4320*8640) converted_env = [] for i in range(4): attr = input_matrix[i,:,:].reshape(input_matrix.shape[1]*input_matrix.shape[2],1) converted_env.append(attr) converted_env = pd.DataFrame(np.hstack(converted_env),columns=title) converted_env[converted_env\u0026lt;0] =0 %%time # Predict the probability of presence of Aedes aegypti for each grid cell with sklearn y_pred =sk_model.predict_proba(converted_env)[::,-1] #CPU times: user 2min 29s, sys: 22.1 s, total: 2min 51s #Wall time: 2min 51s %%time # Predict the probability of presence of Aedes aegypti for each grid cell with cuML y_pred =cuml_model.predict_proba(converted_env)[::,-1] #CPU times: user 1.73 s, sys: 871 ms, total: 2.6 s #Wall time: 2.06 s We can then convert the predicted probability of presence of Aedes aegypti into a raster file. Here, we converted probability estimate with less than 0.3611 as 0 and anything above as 1. The predicted probability \u0026gt; 0.3611 4 are the areas that are expected to have the presence of Aedes aegypti.\n# Convert the predicted probability of presence of Aedes aegypti into a raster file with rasterio.open(\u0026#39;./data/Global Raster/prediction.tif\u0026#39;, \u0026#39;w\u0026#39;,**meta) as dst: # convert to numpy array if the prediction is from cuML dst.write(cp.asnumpy(y_pred).reshape((input_matrix.shape[1],input_matrix.shape[2])),1) # Generate final output which shows grid cells with probability \u0026gt; 0.3611 trigger = y_pred trigger[trigger \u0026gt;=0.36108708] = 1 trigger[trigger \u0026lt; 0.36108708] = 0 with rasterio.open(\u0026#39;./data/Global Raster/trigger.tif\u0026#39;, \u0026#39;w\u0026#39;,**meta) as dst: # convert to numpy array if the prediction is from cuML dst.write(cp.asnumpy(trigger).reshape((input_matrix.shape[1],input_matrix.shape[2])),1) prediction = rasterio.open(\u0026#39;./data/Global Raster/prediction.tif\u0026#39;) trigger = rasterio.open(\u0026#39;./data/Global Raster/trigger.tif\u0026#39;) We can then map the predicted probability of presence of Aedes aegypti and trigger points.\nfig,ax= plt.subplots(2,1,figsize=(20,20)) country_outline.boundary.plot(edgecolor=\u0026#39;white\u0026#39;,linewidth=0.5,ax=ax[0]) show(prediction,cmap=\u0026#39;nipy_spectral\u0026#39;,ax=ax[0]) country_outline.boundary.plot(edgecolor=\u0026#39;white\u0026#39;,linewidth=0.5,ax=ax[1]) show(trigger,cmap=\u0026#39;nipy_spectral\u0026#39;,ax=ax[1]) ax[0].set_axis_off() ax[1].set_axis_off() ax[0].set_title(\u0026#39;Predicted Probability of Presence of Aedes aegypti\u0026#39;,fontsize=20) ax[1].set_title(\u0026#39;Trigger Points\u0026#39;,fontsize=20) plt.show() Figure 8: Predicted probability of presence of Aedes aegypti and trigger points Conclusion # In this chapter, we demonstrated an end-to-end workflow of using GPU to 1) calculate the point in polygon function, 2) train a random forest classification model, and 3) predict the probability of presence of Aedes aegypti. In all three steps, using a GPU for working with raster files in Python can offer significant performance improvements.\nWhat we have shown here is just a small fraction of what we can do with GPU. More importantly, the functions provided here can be easily migrated into existing workflows where the data is too large to be processed on a CPU.\n"},{"id":3,"href":"/GPU-Analytics/docs/about/gpu/","title":"GPU","section":"Overview","content":" Introduction # Over the past few decades, social science research has faced challenges due to a lack of available data, the cost and time needed to conduct surveys, and limitations on computational power for analysis. These issues have been exacerbated by a decline in survey quality as well as an increase in biases in the characteristics of respondents. There was also no guarantee that long-term, time-consuming surveys would continue to be available due to fiscal austerity (Singleton et al. , 2017). However, in recent years, the availability of big data has greatly expanded the resources available to social scientists, allowing them to access a wide range of information about individuals and societies from sources such as social media, mobile devices, and online transactions. The rapid expansion of data is transforming the practice of social science, providing increasingly granular spatial and behavioural profiles about the society and individuals. The difference is not just a matter of scale.\nThe increasing data availability extends beyond the realm of identifying consumer preferences, allowing us to measure social processes and the dynamics of spatial trends in unprecedented detail. For example, Trasberg and Cheshire (2021) used a large scale mobility data from mobile applications to explore the activity patterns in London during lockdown, identifying the socio-spatial fragmentation between urban communities. Similarly, Van Dijk et al. (2020) used an annually updated Consumer Register (LCR) to estimate residential moves and the socio-spatial characteristics of the sedentary population in the UK, overcoming the limitations of the traditional census data. Gebru et al. (2020), on the other hand, used a large scale dataset of images from Google Street View to estimate the demographic makeup of a neighbourhood. Their results suggested a possibility of using automated systems for monitoring demographics may effectively complement labor-intensive approaches, with the potential to measure demographics with fine spatial resolution, in close to real time.\nWith the growing availability of data, processing and analysing large datasets need more computational power than is currently available, with more complex algorithms that need more compute power to run. To overcome this challenge, researchers have turned to a GPU (Graphics Processing Unit) to accelerate massive data parallelism and computation. Therefore, in this chapter, we will introduce the concept of GPU and explain why GPU can be a useful tool for social scientists. In addition, we will provide a brief introduction to the CUDA and RAPIDS suite, which is a GPU-accelerated framework that can be used to accelerate the data analysis process.\nCPU vs GPU # A CPU is a general-purpose processor that is capable of executing a wide range of tasks, including running operating systems, executing programs, and performing calculations. CPUs are typically designed with a focus on sequential processing, meaning they are optimised for executing instructions one at a time and in a specific order. They typically have a relatively small number of processing cores (often ranging from 2 to 32) and are capable of performing a wide range of functions. A GPU, on the other hand, is a specialised processor that is designed specifically for handling graphics and visualisations. GPUs have numerous processing cores (usually hundreds or thousands) and are optimised for parallel processing, meaning they can perform many calculations simultaneously. This makes GPUs particularly well-suited for tasks that require a lot of processing power, such as rendering 3D graphics, running machine learning algorithms, or performing scientific simulations (NVIDIA, 2020).\nThe main difference between a CPU and a GPU is the its architecture (see Figure 1). GPUs dedicate most of their transistors for ALU units (Arithmetic Logic Unit) which are responsible for performing arithmetic and logical operations. CPUs, on the other hand reserve most of their transistors for caches and control units, which aim to reduce latency within each thread. Most CPUs are multi-core processors, meaning they have multiple processing cores that can execute instructions with multiple data streams. This architecture is called Multiple Instruction, Multiple Data (MIMD). This architecture is designed to minimise the time it takes to access data (ref: white bars in Figure 2). In a single time slice, a CPU thread tries to get as much work done as possible (ref: green bar in Figure 2). To achieve this, CPUs require low latency, which is achieved through large caches and complex control logic. However, caches work best with only a few threads per core, as switching between threads is expensive (NVIDIA, 2020).\nGPUs hide instruction and memory latency with computation. GPUs use a Single Instruction, Multiple Data (SIMD) architecture, where each thread is assigned a small amount of memory (blue bar), resulting a much higher latency per thread. However, GPUs have many threads per core, and it can switch from one thread to another at no cost, resulting higher throughput and bandwidth for large data. What this means, in the end, is that we can store more data in the GPU memory and caches, which can be reused for matrix multiplication and operations that are more computationally intensive (NVIDIA, 2020).\nAs shown in Figure 2, when thread T1 is waiting for data, another thread T2 begins processing, and so on with T3 and T4. In the meantime, T1 eventually gets the data it needs to process. In this way, latency is hidden by switching to other available work. As a result, GPUs can utilise overlapping concurrent threads to hide latency and are able to run thousands of threads at once. The best CPUs have about 50GB/s while the best GPUs have 750GB/s memory bandwidth. So the larger your computational operations are in terms of memory, the larger the advantage of GPUs over CPUs.\nWe can make the minions\u0026rsquo; analogy to explain the difference between CPU and GPU. A CPU is like Gru who is considerably intelligent and capable of building fantastic machines, but he can only do one thing at a time. A GPU is like a swarm of minions who are not as intelligent as Gru, but they can do build one thing collectively.\nFigure 1: CPU versus GPU architecture. A CPU devotes more transistors to control the data flow, while GPUs devote more transistors to data processing [SOURCE]. Figure 2: CPU versus GPU processor architecture (NVIDIA, 2020). Are GPUs better than CPUs? # It is not accurate to say that one is always better than the other: both CPUs and GPUs have their own strengths and are optimised for different types of tasks - not all tasks can be accelerated by a GPU. The first thing to consider whether to use a CPU or a GPU for your analysis is the scale of the data. A good GPU can read/write its memory much faster than a CPU can read/write its memory. GPUs can perform calculations much faster and more efficient to process massive datasets than the host CPU. An example of matrix multiplication executed on a CPU and a GPU is shown in Figure 3: the CPU can be faster when the matrix size is small, but the GPU is much faster when the matrix size is large. This also applies to other data science tasks such as training a neural network. You may get away without a GPU if your neural network is small in scale. However, it might be worthwhile to consider investing in a GPU if the neural network involves hundreds of thousands of parameters and requires extensive training times.\nFigure 3: Computational speed between CPU and GPU (MathWorks, 2022). A second thing to consider when deciding between architecture relates to the type of project. If you are working on a project that requires a lot of parallel processing, such as rendering large spatial objects, running machine learning algorithms, or performing scientific simulations, a GPU is a good choice. However, if you are working on a project that requires a high level of sequential processing, such as running multiple functions in a loop, a CPU is a good choice that offers more flexibility. The third thing to keep in mind is general accessibility. Access to GPUs is not as easy as CPUs. GPUs are usually more expensive than CPUs, and they are not as widely available.\nMore generally, if you are working with a Cloud GPU providers such as Google Colab, Microsoft Azure, or Amazon AWS, the cost of GPU per hour would be somewhere between $0.5 and $3.5. If you are working with a local GPU, the average cost is around $450. Furthermore, you need to make sure that your GPU is compatible with the CUDA toolkit. M1 chips, for example, are not compatible with CUDA toolkit, prohibiting some GPU computing libraries from being used.\nWhy have GPUs become more popular? # Over the past few years, GPUs have become more and more popular in data science. This is mainly due to the development of the The NVIDIA® CUDA® Toolkit, which simplifies GPU-based programming. CUDA (Compute Unified Device Architecture) is a general purpose parallel computing platform and application programming interface (API) developed by NVIDIA. CUDA uses a hybrid data processing model where serial sections of code run on the CPU and parallel sections run on the GPU (see Figure 4).\nCUDA gives us direct access to the GPU’s virtual instruction set and parallel computational elements, making it easier for compiling compute kernels into code that will execute efficiently on the GPU. In CUDA, the computation is divided into small units of work called threads. Threads are grouped into thread blocks, and multiple thread blocks can be executed simultaneously on the GPU. Thread blocks are further grouped into grids, which represent a collection of thread blocks that can be executed in parallel on the GPU.\nIn CUDA, the function (kernel) is executed with the aid of threads. Each thread performs the same operation on different elements of the input data, allowing the GPU to perform the computation in parallel. A block of threads is controlled by the streaming multiprocessing unit that can be scheduled and executed concurrently. Several blocks of threads can be grouped into a grid, which constitutes the whole GPU unit.\nThe relationship between grid, thread, and block in CUDA can be visualised as a three-dimensional structure, with each thread block representing a plane and each grid representing a collection of planes. The number of threads per block determines the width of the plane, and the number of blocks per grid determines the number of planes in the collection. The GPU schedules and executes the threads in the grid, and each thread performs its computation on a different element of the input data.\nFigure 4: How GPU acceleration works [SOURCE]. The CUDA toolkit and many third-party libraries offer a collection of well-optimised and pre-compiled functions that enable drop-in acceleration across multiple domains such as linear algebra, image and video processing, deep learning, and graph analytics. For developing custom algorithms, you can use available integrations with commonly used languages and numerical packages, as well as well-published development API operations. Some widely used libraries are:\nMathematical libraries: cuBLAS, cuRAND, cuFFT, cuSPARSE, cuTENSOR, cuSOLVER Parallel algorithm libraries: nvGRAPH, Thrust Image and video libraries: nvJPEG, NPP, Optical Flow SDK Communication libraries: NVSHMEM, NCCL Deep learning libraries: cuDNN, TensorRT, Riva, DALI Partner libraries: OpenCV, FFmpeg, ArrayFire, MAGMA As a social scientist, you may not need to use all of these libraries. However, it is important to know that these libraries exist and are available for you to use. Libraries such as RAPIDS, Numba or PyTorch offer a high level programming interface that allows you to use GPU computing without having to learn C/C++ or CUDA. Furthermore, library such as RAPIDS cuDf and CuPy provide a drop-in replacement for Pandas and Numpy allowing you to use GPU computing for tasks such as data cleaning, feature engineering, and machine learning without having to change much of your code. Some libraries that fall under the RAPIDS banner with their CPU-based counterparts are:\ncuDF (Pandas) cuML (Sklearn) cuGraph (NetworkX) cuSpatial (Geopandas/Shapely) CuPy (Numpy) It is important to note that CUDA is not the only parallel computing platform. With the rise of GPU computing, companies such as AMD and Intel have developed their own computing framework. OpenCL, for instance, is another parallel computing platform and programming model that is supported by AMD and Intel. However, at the moment CUDA is the most popular parallel computing platform and it is the only one that is supported by NVIDIA.\n"},{"id":4,"href":"/GPU-Analytics/docs/preface/about/","title":"Preface","section":"Docs","content":" GPU-based analysis for social and geographic applications # Introduction # This document provides the tools, the code and utility of the graphics processing unit (GPU) for social and geographic research and applications.This project is funded by The British Academy\u0026rsquo;s Talent Development Award.\nIn most cases data science tasks are executed on local servers or personal devices where calculations are handled by one or more Central Processing Units (CPUs). CPUs can only handle one task at the time, meaning that the computational time for millions of sequential operations can only be sped up by adding more CPUs and parallelising these calculations. A Graphics Processing Units (GPU), on the other hand, is designed to execute several tasks simultaneously. GPU-accelerated analytics harness the parallel-capabilities of the GPU to accelerate processing-intensive operations. This can be particularly useful for social and geographic research where the data sets are often large and complex.\nIn this document, we will explore a collection of novel and innovative computational methods in social and geographic data science. This is essential for at least two reasons. First, with the proliferation of large-scale data sets as well as the availability of increasingly powerful personal computing devices over the past decade or so, social science and geography have witnessed a second quantitative revolution - Kitchen (2014, p.3) speaks of a fourth paradigm of science with a focus on data-intensive analyses, statistical exploration and data mining. However, there is limited availability of well-documented resources for both social and geographical data science researchers and students in handling large volumes of data in an efficient manner.\nSecond, whereas ‘Big Data’ can be very rich sources of information, they tend to be accidental (e.g. a by-product of online transactions) and highly diverse in quality and resolution. As a result, many ‘Big Data’ sources are not representative of the population or phenomenon of study and contain a variety of biases and uncertainties. An illustrative example of the problem is described in Van Dijk et al. (2021) where, in the absence of frequently updated data on the nature of residential moves in the United Kingdom, the authors use population registers and administrative data to develop robust annual estimates of residential mobility between all UK neighbourhoods by ascribing individuals that seemingly vacate a property to their most probably residential destination. With circa 7.8 billion possible origin-destination pairings, this was a very time-consuming and computationally intensive model.\nThis raises the question of how technological innovations can be harnessed for the benefit of social and geographic data science research: both to enable future highly computationally intensive research as well as how to effectively communicate these new research pipelines to researchers and students within the domains of computational social sciences and quantitative geography.\nWho is this document for? # In this document, we will explicitly explore the potential of GPU with three case studies, and best practices for GPU in social and geographic research. This document is aimed at researchers and students in the domains of computational social sciences and quantitative geography who are interested in learning and using GPU as part of their research. It is also aimed at data science and machine learning practitioners who are interested in working with geographical data and problems in a more efficient manner. While some content is aimed at a more technical audience, the document is written in a way that is accessible to a wide audience.\nWe also want to highlight that this document is not a comprehensive guide to GPU and GPU programming. Rather, it is a collection of resources and projects that can be used to explore the potential of GPU-based analysis in social and geographic research.\nDocument details # In this project we will first explain the basics of GPU and how to configure GPU both locally and on the cloud. In this project, we will be using a NVDIA Tesla V100 GPU for our demonstrations, but the code and instructions can be easily adapted to other GPUs.\nAfter configuration, the usability of GPU-accelerated analytics in the context of three different geographic applications will be benchmarked through three case studies. The first of these case studies will explore the matching and joining of large textual datasets comprising millions of addresses. The second case study will explore the performance of deep learning algorithms for GeoAI applications. The third case study will look specifically at the opportunities for improving the performance of disease risk prediction using raster data. The final deliverable of the project will be a workshop alongside a freely available, shareable digital resource with detailed instructions on how to implement GPU-based analysis in social and geographic research and teaching.\nProject members # Dr Justin van Dijk Dr Stephen Law Dr Anwar Musah Mr Jason Chi Sing Tang License # This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\nProject reference: # TDA21\\210069\n"},{"id":5,"href":"/GPU-Analytics/docs/about/setting_up/","title":"Setting up the environment for GPU","section":"Overview","content":" Introduction # Setting up the environment for GPU can be challenging, as every computer has different hardware and software configurations. There are no universal instructions that will work for everyone, but in this chapter, we will discuss how to set up the environment using Google Colab as well as a GPU on a remote server. We will also highlight the steps to verify that the GPU is working properly.\nGetting started: Google Colab # Google Colab is a cloud-based platform that allows you to run Jupyter notebooks in the Cloud with support for GPU-acceleration. Google Colab is free to use, and you do not need to install any software or setup CUDA manually. It comes with already pre-installed libraries such as Pandas or NumPy, so you do not need to run pip install by yourself.\nHow to start? # The first step is to create a new notebook in Google Colab. You can do this by going to the Google Colab website, signing in with your Google account, and creating a new notebook.\nFigure 1: Create a new notebook in Google Colab. Enabling GPU support # After creating a new notebook, you need to enable GPU support. To do this, go to the Runtime menu and select Change runtime type. Then, set the Hardware accelerator to GPU.\n\u003e Figure 2: Select Change runtime type in the Runtime menu. After you change your runtime type, your notebook will restart, which means information from the previous session will be lost.\nFigure 3: Select GPU in the Hardware accelerator menu. You may also see the TPU option in the drop-down menu. TPU stands for a Tensor Processing Unit, which is a specialised chip designed to accelerate machine learning workloads. TPU is more powerful than GPU, but it is also more expensive.\nVerify GPU access # After you enable GPU support, you can verify that the GPU is working properly by running the code below in a code block in your notebook.\n!nvidia-smi nvidia-smi (NVIDIA System Management Interface) is a tool to query, monitor and configure NVIDIA GPUs. It ships with and is installed along with the NVIDIA driver and it is tied to that specific driver version. It is a tool written using the NVIDIA Management Library (NVML), which you can also find the Python bindings in the PyPI package pynvml.\nThis outputs a summary table (see Figure 4 for an example output), where you will find the following useful information:\nName: The name of the GPU. Memory: The amount of memory available on the GPU. Utilisation: The percentage of time the GPU is being used. Power usage: The power usage of the GPU. Processes: List of processes executing on the GPUs. Figure 4: NVIDIA System Management Interface output. In this example, we got a Tesla T4 GPU with 16GB of memory. The GPU is currently being used by the notebook, but we can also see that there are no other processes running on the GPU. There is several other ways to verify that the GPU is working, for instance, directly with the torch library by running the following code:\nimport torch use_cuda = torch.cuda.is_available() #Check if GPU is available if use_cuda: print(\u0026#39;__CUDNN VERSION:\u0026#39;, torch.backends.cudnn.version()) #Get the version of cudnn print(\u0026#39;__Number CUDA Devices:\u0026#39;, torch.cuda.device_count()) #Get the number of available GPUs print(\u0026#39;__CUDA Device Name:\u0026#39;,torch.cuda.get_device_name(0)) #Get the name of the GPU print(\u0026#39;__CUDA Device Total Memory [GB]:\u0026#39;,torch.cuda.get_device_properties(0).total_memory/1e9) #Get the total amount of memory available on the GPU Best practices # Google Colab is a great tool for running GPU-accelerated notebooks in the cloud. However, there are some limitations that you should be aware of.\nNotebooks are not persistent: When you create a notebook in Google Colab, it is stored in the Cloud. However, the notebook is not persistent. This means that if you close the notebook, the notebook will be deleted. If you want to save your notebook, you need to download it to your local machine.\nGoogle Colab is not an unrestricted resource: Google Colab is free to use, but they limit the amount of time you can use the GPU. Google Colab will automatically disconnects the notebook if we leave it idle for more than 30 minutes, meaning that your training will be interrupted and you will have to restart it. Nevertheless, you can run the free of charge notebooks for at most 12 hours at a time as explained in the FAQ. If you want to use the GPU for more than 12 consecutive hours or require access to more memory, you can consider switching to the paid version of Colab.\nWorking with data: You can access your files that are stored on Google Drive directly from Google Colab. To do this, you need to mount your Google Drive to the notebook. You can do this by running the following code in a code block in your notebook. Use the batch command !cd the Files panel on the left to access your files.\nfrom google.colab import drive drive.mount(\u0026#39;/content/drive\u0026#39;) You can also upload or download a file (or files) from/to your computer using the following code.\nfrom google.colab import files #Upload a file from your computer files.upload() #Download a file to your computer files.download(\u0026#39;path/to/your/file\u0026#39;) Ensure all files have been completely copied to your Google Drive: As mentioned above, Colab can and will terminate your session due to inactivity. To ensure that your final model and data is saved to your Google Drive, call drive.flush_and_unmount() before you terminate your session. Furthermore, you can checkpoint your model during training process and save it to your Google Drive as well.\nfrom google.colab import drive model.fit(...) # Training your model model.save(\u0026#39;path/to/your/model\u0026#39;) # Save your model drive.flush_and_unmount() print(\u0026#39;All changes made in this Colab session should now be visible in Drive.\u0026#39;) Note that the completion of copying/writing files to /content/drive/MyDrive/ does not mean that all files are safely stored on Google Drive and that you can immediately terminate your Colab instance because the transfer of data between the Virtual Machine on which you run your analysis and Google\u0026rsquo;s infrastructure happens asynchronously, so performing this flushing will help ensure it is indeed safe to disconnect.\nDisconnecting from the notebook: Be a responsible user and disconnect from the notebook when you are finished with your GPU. Google will notice and reward you with a better GPU the next time you request one! So after you have copied your saved model files to Google Drive or Google Storage, you can disconnect from the notebook by clicking the Disconnect button in the Runtime menu. Or you can run the following code in a code block in your notebook.\nfrom google.colab import runtime runtime.unassign() Getting started: Remote server # At UCL, Department of Geography, we have a dedicated GPU-server that we can use to run GPU-accelerated notebooks on a Tesla V100 GPU. In theory, you should be able to connect to the server from anywhere in the world and run your GPU analyses - although access to the server needs to be requested. Setting up the environment on the server is a bit more complicated than setting up on cloud-based solutions such as Google Colab. For the below to work you will need three things: access to the server, a correct installation of CUDA, and an installation of the conda package manager.\nssh into the server # If you have access to a dedicated GPU-server, you can log into the server using the following bash command:\nssh username@server_ip port After this command, you will be prompted to enter your password.\nInstalling libraries # You can initialise a new conda environment using the following bash command.\nconda create -n gpu -c rapidsai -c nvidia -c conda-forge rapids=22.06 python=3.9 cudatoolkit=11.5 jupyterlab This will create a new conda environment called gpu and install the CUDA toolkit with some of the essential libraries that you will need.\nActivate the environment # You can activate the environment using the following bash command.\nconda activate gpu Start the JupyterLab server # You can start the JupyterLab server using the following bash command.\njupyter lab --no-browser This will start the JupyterLab server and you will be able to access it from your browser.\nGetting started: Local machine # If your local machine has a supported GPU, you might be able to install the CUDA toolkit. Most of GPU-accelerated libraries build on top of the CUDA framework. Therefore, you need to have a GPU that supports CUDA. You can check the CUDA GPU support matrix to see if your GPU is supported. Of course, you should have a decent CPU, RAM and Storage to be able to do some leverage the power of the GPU.\nFor the minimum hardware requirements, we recommend the following:\nCPU: Minimum 4 cores at 2.6GHz; at least 16GB of RAM. GPU: NVIDIA GPU with at least 6GB of VRAM. As installations and setting up instructions depend on your operating system and hardware, please refer to the official documentation.\n"}]