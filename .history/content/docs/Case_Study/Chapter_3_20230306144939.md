---
title: "Chapter 3 - Geospatial Operation and Analysis" 
weight: 1
# bookFlatSection: false
# bookToc: true
# bookHidden: false
# bookCollapseSection: false
# bookComments: false
# bookSearchExclude: false
---


## Introduction 

Why are we interested in geospatial data? Geospatial data is a type of data that is associated with a location. This location can be a point, a line, a polygon, or a raster. Geospatial data is becoming more and more important, yet the sheer volume of information that is generated made it difficult to handle. In this chapter, we will be exploring the use of GPU for manipulating large-scale geospatial data, and provide a practical example of how we can predict the presence of Aedes aegypti across the globe. 
## Objectives 

The objective of the third *Case Study* is to demonstrate the practical application of the common spatial data structures and operation with GPU-accelerated Python packages. The goal here is in twofold: 1) compare the computational speed of calculating point in polygon and 2) compare the computational speed of a classification model with raster data. 

## Predicting the Presence of Aedes aegypti 
In this case study, we will be predicting the global presence and probability of Aedes aegypti, a mosquito species that is known to transmit dengue fever, chikungunya, and Zika virus. You can download the Aedes aegypti point occurrence data across the World from 1958 to 2014 [here](https://github.com/jasoncpit/GPU-Analytics/blob/master/data/Chapter3/aedes_point.csv). We will also be using precipitation, temperature, elevation, and population density as predictor variables to capture the climatic, environmental and demographics variables. You can download the raster data from the GitHub repository [here](https://github.com/jasoncpit/GPU-Analytics/tree/master/data/Chapter3).

### Load datasets 
To begin, let's load the necessary libraries and datasets. It is important to note that all shape file and raster data (5km resolution) were projected to the CRS: WGS84 4236.
```python
#Import libraries
import rasterio
import geopandas as gpd 
import numpy as np 
import pandas as pd 
import seaborn as sns 
import matplotlib.pyplot as plt 
import cuspatial
import cudf

# Load raster data 
precipitation = rasterio.open('./data/Global Raster/Precipitation/Global Averaged Precipitation 2000-14.tif')
temp = rasterio.open('./data/Global Raster/Temperature/Global Averaged Temperature 2000-14.tif')
elevation = rasterio.open('./data/Global Raster/Elevation/Global Land Surface Elevation.tif')
pop_density = rasterio.open('./data/Global Raster/Population Density/Global Population Density AveragedEst.tif')

# Load Shapefile 
#Loading global shapefile 
global_outline =gpd.read_file('./data/Global Shapefile/Global Outline/Global_All_0.shp',crs='EPSG:4326')
#Loading country shapefiles 
country_outline = gpd.read_file('./data/Global Shapefile/Global with Country Outline/Global_Countries_1.shp',crs='EPSG:4326') 

# Load Point occurrence data 
aedes =pd.read_csv('./data/Global Raster/aedes_point.csv')
aedes_point = gpd.GeoDataFrame(aedes,geometry=gpd.points_from_xy(aedes['longitude'],aedes['latitude']),crs='EPSG:4326')

# Transformation 
global_outline.crs = ('+init=EPSG:4326')
country_outline.crs =('+init=EPSG:4326')  
aedes_point.crs = ('+init=EPSG:4326')

# Check CRS 
print(global_outline.crs ==aedes_point.crs)
```

### Data Visualization 

Let's visualize the point occurrence data and the predictor variables. We will be using the `matplotlib` and `rasterio` packages to visualize the data. 
```python
# Visualise point patterns 
import matplotlib.pyplot as plt 
fig, ax = plt.subplots(1,1, figsize = (10,10)) 
country_outline.plot(edgecolor='black',linewidth=0.1,ax=ax,color="white")
aedes_point.plot(ax=ax,color='red',markersize = 1) 
#show(light,ax=ax,cmap='viridis')
ax.axis('off')
ax.set_title("Distribution of aedes occurence across the World")
```

<figure title = "Global Distribution">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp3_global.png?raw=true">
    <figcaption>
    <b>Figure 1: Global Distribution of Aedes aegypti 
    </b>
    </figcaption>
    </center>
</figure>

```python
from rasterio.plot import show 
fig, ax = plt.subplots(nrows=2,ncols=2, figsize=(20,20))
title =["precipitation","temp","elevation","pop_density"] 
for index,attribute in enumerate([precipitation,temp,elevation,pop_density]): 
     image = show(attribute,ax=ax[index//2,index%2],cmap='nipy_spectral',title = title[index])
     image.axis('off')
fig.subplots_adjust(hspace=-0.5, wspace=0)
```

<figure title = "Predictors Distribution">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp3_predictors.png?raw=true">
    <figcaption>
    <b>Figure 2: Distribution of Predictors: Precipitation, Temperature, Elevation, and Population Density 
    </b>
    </figcaption>
    </center>
</figure>


### Prepare data for pseudo-background points as absence 

Next, we need to prepare the background data. What is the background data? With Background data we are not attempting to guess point locations where an event is absent. Here, we are rather trying to characterise the environment of the study region. In this sense, background is the same, irrespective of where the point fire are found or not. Background data establishes the environmental domain of the study, whilst presence data should establish under which conditions a fire is more likely to be present than on average. 

There are several ways to generate background data. In R, we can use the `spsample()` function from the `sp` package to generate randomly-distributed points with defined spatial boundaries. However, in Python, there is no pre-built function to help us. Instead, we will generate random points across the world and filter out the points that are outside of the country boundary. Here is the code to generate random points across the world.

```python
# random seed 
import random
from shapely.geometry import Point
from tqdm import tqdm
random.seed(10)
def Random_Points_in_Polygon(polygon, number):
    bound = polygon.bounds.values[0]
    minx, miny, maxx, maxy = bound[0],bound[1],bound[2],bound[3]
    x_point,y_point = np.random.uniform(minx, maxx,size=number),np.random.uniform(miny,maxy,size=number) 
    return x_point,y_point
background_points = Random_Points_in_Polygon(global_outline,aedes_point.shape[0]*5)
background_points_shp = gpd.GeoDataFrame(geometry=gpd.points_from_xy(x=background_points[0],y=background_points[1]),crs='EPSG:4326')
background_points_shp.crs = global_outline.crs 
print("Number of background points: ",background_points_shp.shape[0]) 
#Number of background points:  171108 
```

Figure 3 shows the distribution of background points across the world. We generated 171,108 points that are evenly distributed across the world, with some points located in the ocean. 

```python
# Visualise point patterns 
fig, ax = plt.subplots(1,1, figsize = (10,10)) 
global_outline.plot(edgecolor='black',linewidth=2,ax=ax,color="white")
background_points_shp.plot(ax=ax,color='blue',markersize = 0.1) 
ax.axis('off')
ax.set_title("Distribution of background points across the World")
```


<figure title = "Predictors Distribution">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp3_background_point.png?raw=true">
    <figcaption>
    <b>Figure 3: Generating pseudo-background points as absence 
    </b>
    </figcaption>
    </center>
</figure>


### Calculate point in polygon with GeoPandas and CuSpatial

Now that we've created the background points, we need to quickly determine which points occur in each country. This task is commonly known as a Point in Polygon (PIP) query, however, they are usually extremely slow. In Python, we can use the Shapely library's functions `.within()` and `.contains()` to determine if a point is within a polygon or if a polygon contains a point. Nevertheless, these functions are not designed to handle large datasets. Assuming that we have 1 million points and 100 polygons, the `.within()` function will take 100 million comparisons to determine which points are within which polygons. This is a very slow process.

Let's first use the `.sjoin()` function from GeoPandas to determine which points are within which polygons. Using the `%%time` magic function, we can see that the `.sjoin()` function takes 8.6 seconds to compute 171108 points. 

```python
# pip query with geopandas 
%%time 
pointInPolys = gpd.sjoin(background_points_shp, country_outline) 
#CPU times: user 8.6 s, sys: 43 ms, total: 8.65 s
#Wall time: 8.69 s
```
Next, we can use the `.point_in_polygon` function from the GPU-accelerated library `cuSpatial`. It is important to note here that the `.point_in_polygon` requires the points to be in the form of a `cudf.DataFrame` and the polygons to be in the form of a `cuspatial.GeoSeries`. Here, we are using the `cuspatial.read_polygon_shapefile()` function to make sure the resulting tuple `poly_offsets`, `poly_ring_offsets`, `poly_points` perfectly matches the input requirements of `point_in_polygon`. 

Because the `.point_in_polygon` function can only handle 31 polygons at a time, we need to split the polygons into batches of 31 polygons. The for loop shown below iterates through each batch and append true values in the array to a new Country ID, matching the spatial indices of the polygons.

The `.point_in_polygon` function performs the PIP query in 0.5 seconds, which is 16 times faster than the `.sjoin()` function. 

```python
%%time
# Cuspatial 
cu_countries_outline = cuspatial.read_polygon_shapefile('./data/Global Shapefile/Global with Country Outline/Global_Countries_1.shp')

background_points_df = cudf.DataFrame() 
background_points_df['Long'] = background_points[0]
background_points_df['Lat'] = background_points[1]
background_points_df['LocationID'] =  cu_countries_outline[0].shape[0] 
pip_iterations = list(np.arange(0, cu_countries_outline[0].shape[0], 31))

for i in range(len(pip_iterations)-1):
    start = pip_iterations[i]
    end = pip_iterations[i+1]
    pip_countries = cuspatial.point_in_polygon(background_points_df['Long'],background_points_df['Lat'],cu_countries_outline[0][start:end],cu_countries_outline[1],cu_countries_outline[2]['x'],cu_countries_outline[2]['y'])
    for j in pip_countries.columns:
        background_points_df['LocationID'].loc[pip_countries[j]] = j
#CPU times: user 553 ms, sys: 8.88 ms, total: 562 ms
#Wall time: 563 ms         
```

### Stress Test 

The differences may be marginal at this point, but the `.point_in_polygon` function will become more significant as the number of background points increases. We can run a stress test by increasing the number of background points. The code below generates 10-30 times more background points than the original dataset. We can see from Figure 4 that the `.sjoin()` function takes linearly longer to compute the PIP query, while the `.point_in_polygon` function takes a constant time to compute the PIP query, with average runtimes of less than 2.5 seconds.  

```python
import time
cpu_time = []
gpu_time = []

for i in range(10,30):
    #Generate random background points
    background_points = Random_Points_in_Polygon(global_outline, aedes_point.shape[0] * i)

    #Preparing data on Geopandas 
    background_points_shp = gpd.GeoDataFrame(geometry=gpd.points_from_xy(x=background_points[0], y=background_points[1]), crs='EPSG:4326')
    background_points_shp.crs = global_outline.crs
    #cpu runtime -------------------------------- 
    start = time.time()
    CPU_pointInPolys = gpd.sjoin(background_points_shp, country_outline)
    end = time.time()
    cpu_time.append(end - start)

    #Preparing data on cudf 
    background_points_df = cudf.DataFrame()
    background_points_df['Long'] = background_points[0]
    background_points_df['Lat'] = background_points[1]
    background_points_df['LocationID'] = cu_countries_outline[0].shape[0]
    pip_iterations = list(np.arange(0, cu_countries_outline[0].shape[0], 31))

    #gpu runtime --------------------------------
    start = time.time()
    for iter in range(len(pip_iterations) - 1):
        pip_start = pip_iterations[iter]
        pip_end = pip_iterations[iter + 1]
        pip_countries = cuspatial.point_in_polygon(background_points_df['Long'], background_points_df['Lat'], cu_countries_outline[0][pip_start:pip_end], cu_countries_outline[1], cu_countries_outline[2]['x'], cu_countries_outline[2]['y'])
        for j in pip_countries.columns:
            background_points_df['LocationID'].loc[pip_countries[j]] = j
    end = time.time()
    gpu_time.append(end - start)

```

```python 
#Create a dataframe to store the results
gpu_elapsed = pd.DataFrame({"time":gpu_time,"data_size":[aedes_point.shape[0]* i for i in range(10,30)],'label':"cuspatial.point_in_polygon"})
cpu_elapsed = pd.DataFrame({"time":cpu_time,"data_size":[aedes_point.shape[0]* i for i in range(10,30)],'label':"gpd.sjoin"})
result = pd.concat([gpu_elapsed,cpu_elapsed]).reset_index()

#Plot results
fig, ax = plt.subplots(figsize=(10,10))
sns.lineplot(x= 'data_size',y='time',hue = 'label',data = result,ax = ax )
plt.xlabel('Data Size')
plt.ylabel("Time Elapsed ")
plt.title("Comparing the speed of Point-in-Polygons calculation on CPU and GPU") 
plt.show()
```


<figure title = "PIP SPEED">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp3_pip.png?raw=true">
    <figcaption>
    <b>Figure 4: Comparing the speed of Point-in-Polygons calculation on CPU and GPU 
    </b>
    </figcaption>
    </center>
</figure>


### Visualize point patterns 

Now that we know how to calculate the PIP query, we can visualize the point patterns by selecting the background points that are within the polygon of a country. The code below visualizes the point patterns of the countries with the highest number of cases. 

```python
# Visualise point patterns

#Recalculate the PIP query with Cuspatial 
background_points = Random_Points_in_Polygon(global_outline,aedes_point.shape[0]*5)
background_points_df = cudf.DataFrame() 
background_points_df['Long'] = background_points[0]
background_points_df['Lat'] = background_points[1]
background_points_df['LocationID'] =  cu_countries_outline[0].shape[0] 
pip_iterations = list(np.arange(0, cu_countries_outline[0].shape[0], 31))

for i in range(len(pip_iterations)-1):
    start = pip_iterations[i]
    end = pip_iterations[i+1]
    pip_countries = cuspatial.point_in_polygon(background_points_df['Long'],background_points_df['Lat'],cu_countries_outline[0][start:end],cu_countries_outline[1],cu_countries_outline[2]['x'],cu_countries_outline[2]['y'])
    for j in pip_countries.columns:
        background_points_df['LocationID'].loc[pip_countries[j]] = j

pointInPolys = background_points_df.query("LocationID != 250")  
pointInPolys = gpd.GeoDataFrame(geometry = gpd.points_from_xy(pointInPolys['Long'].to_numpy(),pointInPolys['Lat'].to_numpy()),crs="EPSG:4326")
import matplotlib.pyplot as plt 
fig, ax = plt.subplots(1,1, figsize = (10,10)) 
global_outline.plot(edgecolor='black',linewidth=1,ax=ax,color="white")
pointInPolys.plot(ax=ax,color='blue',markersize = 0.1) 
ax.axis('off')
ax.set_title("Distribution of background points across Brazil")
```


<figure title = "Predictors Distribution">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp3_filtered_background.png?raw=true">
    <figcaption>
    <b>Figure 5: Filtering the pseudo-background points that are within the country boundary
    </b>
    </figcaption>
    </center>
</figure>
```

### Create a multi-band raster from the predictor variables 

To facilitate the required analysis, it is necessary to construct a multi-band raster object from the predictor variables. A band is essentially a matrix of cell values, and a raster with multiple bands comprises multiple matrices of cell values that overlap spatially and represent the same geographic region. For example, the raster object for temperature is a single-band raster object. However, if we stack raster objects for variables such as precipitation, population density and elevation on top of the temperature raster, we create a multi-band raster object. This object will have four bands, each corresponding to a single matrix of cell values.

The creation of this multi-band raster object is essential to perform the extraction of raster values from all variables at occurrence points in a single step. Additionally, the entire multi-band raster object is required for estimating and predicting spatial data.

```python
meta = precipitation.meta
meta.update(count = 4)
with rasterio.open('./data/Global Raster/global_stack.tif', 'w',**meta) as dst:
    for index,attribute in enumerate([precipitation,temp,elevation,pop_density],start=1): 
        dst.write(attribute.read(1),index) 
stack = rasterio.open('./data/Global Raster/global_stack.tif')         
```


### Extraction of all raster values from predictor variables onto presence-absence points 

Now, we are going to extract information from our raster stack to both the presence and background points. This can be done using the sample function in rasterio. For all occurrence points (i.e., presence), we need to add an indicator of 1 to signify presence; while for all background points (i.e., absence) - we need to also add an indicator of 0 to signify absence. We do this because we are modelling a probability and such niche models take outcomes that are from a Bernoulli or Binomial distribution.

```python
# Extrat raster values 
background_list = [(x,y) for x,y in zip(pointInPolys['geometry'].x , pointInPolys['geometry'].y)]
pointInPolys['value'] = [x for x in stack.sample(background_list)]

aedes_list = [(x,y) for x,y in zip(aedes_point['geometry'].x , aedes_point['geometry'].y)]
aedes_point['value'] = [x for x in stack.sample(aedes_list)]

# Convert into dataframe 
aedes_env = pd.DataFrame(np.vstack(aedes_point['value']),columns=title)
aedes_env['Presence'] = 1
background_env = pd.DataFrame(np.vstack(pointInPolys['value']),columns=title)
background_env['Presence'] = 0 

#Merge 
input_data = pd.concat([aedes_env,background_env],axis=0)
input_data[input_data<0]  =0
```
#Save

### Preparation of training & test data for prediction & model cross-validation

Now, we need to prepare our data for model cross-validation. We are going to split our data into training and test data. The training data will be used to train the model, while the test data will be used to validate the model. The test data will be used to assess the model’s performance on data that it has not seen before. We are going to use a 80:20 split for training and test data, respectively.

```python
#Split train,test set 
from sklearn.model_selection import train_test_split
y= input_data.pop('Presence')
X = input_data 
X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8)
```

### Random Forest Classification Model with CuML and Scikit-learn

```python
# Random forest with sklearn 
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(max_depth=20, random_state=42,n_estimators=100)
clf.fit(X_train,y_train)
y_pred =clf.predict(X_test) 
``` 

### Examination of the predictor’s contribution and Model Validation
<figure title = "Predictors Distribution">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp3_Feature_importance.png?raw=true">
    <figcaption>
    <b>Figure 5: Feature importance of the Random Forest Classification Model
    </b>
    </figcaption>
    </center>
</figure>

<figure title = "Predictors Distribution">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp3_ROC_curve.png?raw=true">
    <figcaption>
    <b>Figure 6: ROC Curve of the Random Forest Classification Model
    </b>
    </figcaption>
    </center>
</figure>



### Mapping the predicted probability of presence of Aedes aegypti 

### Forecasting the presence of Aedes aegypti in 2050 

### Conclusion 
