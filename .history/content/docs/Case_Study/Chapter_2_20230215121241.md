---
title: "Chapter 2 - GeoAI and Deep Learning"
weight: 1
# bookFlatSection: false
# bookToc: true
# bookHidden: false
# bookCollapseSection: false
# bookComments: false
# bookSearchExclude: false
---


## GeoAI and Deep Learning 

GeoAI, or geospatial artificial intelligence (AI), has become a trending topic and the frontier for spatial analytics in Geography [(Li and Hsu, 2022)](https://www.mdpi.com/2220-9964/11/7/385/pdf). Although the field of AI has experienced highs and lows in the past decades, it has recently gained tremendous momentum because of breakthrough developments in deep (machine) learning, immense available computing power, and the pressing needs for mining and understanding big data. 


## Objectives 

The objective of the second *Case Study* is to showcase how we can use GPU for satellite image classification. We will be discussing two case studies - (1) training a CNN model from scratch using Pytorch to detect land use classification from satellite images (2) using a pretrained computer vision model to understand the "scenicness" of images. While using a GPU is a commonly integrated into deep learning libraries, we will also provide best practices for maximizing your training efficiency. 


## Case Study 1: Classifying EuraSat images using Convolutional Neural Networks (CNNs)

In this case study, we will be using the EuraSat dataset to train a CNN model to classify land use from satellite images. The EuraSat dataset contains 27,000 images of 10 different land use classes. The dataset is available on Kaggle and can be downloaded [here](https://www.kaggle.com/phylake1337/eurasat-land-use-and-land-cover). The dataset is also available on the [PyTorch website](https://pytorch.org/vision/stable/datasets.html#eurasat).

### Brief introduction to Convolutional Neural Networks (CNNs) 

Convolutional Neural Networks (CNNs) are a type of artificial neural network that are designed to work with grid-structured data, such as an image, a speech signal, or a video. They are particularly effective for image and video classification, object detection and recognition, and natural language processing tasks.

The key components of a CNN are convolutional layers, activation functions, pooling layers, and fully connected layers. 


1. Convolutional layers: Convolutional layers are the building blocks of a CNN. They perform a convolution operation on the input data, where a small matrix (known as a filter or kernel) is moved across the input data, element-wise multiplication is performed between the elements of the filter and the input data, and then the results are summed up to produce a single output value. This process is repeated for every possible position of the filter, resulting in a set of outputs, called feature maps. Convolutional layers can extract features from the input data, such as edges, shapes, textures, etc.

2. Activation functions: Activation functions are used to introduce non-linearity into the network. They are applied element-wise to the output of the convolutional layer. The most commonly used activation functions in CNNs are Rectified Linear Unit (ReLU) and sigmoid.

3. Pooling layers: Pooling layers are used to reduce the spatial size of the feature maps, making the network less computationally expensive and more robust to changes in the position of objects in the input data. There are several types of pooling, including max pooling and average pooling. In max pooling, the maximum value in a region of the feature map is taken as the output, while in average pooling, the average value in a region is taken as the output.

4. Fully connected layers: The fully connected layers are used to make the final prediction using the features extracted by the convolutional and pooling layers. They perform a weighted sum of the inputs, followed by a non-linear activation function, and then produce the final output of the network.


The architecture of a CNN can be designed for a specific task by choosing the number of convolutional and fully connected layers, the size of the filters, the type of activation functions, and the type of pooling. The weights of the filters and the biases of the fully connected layers are learned from the training data using an optimization algorithm, such as stochastic gradient descent or Adam. 


A classic CNN architecture would look something like this (Figure 1): 


<figure title = "CPU tensorboard">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp2_cnn.png?raw=true">
    <figcaption>
    <b>Figure 1: Framework of a Convolutional Neural Network (Illustration by [Mathworks](https://uk.mathworks.com/help/deeplearning/ug/introduction-to-convolutional-neural-networks.html)]</b>
    </b>
    </figcaption>
    </center>
</figure>


For more information on CNNs, you can check out this cool [blog post](https://poloclub.github.io/cnn-explainer/).



### Step 1: Importing the libraries 

We will be using the following libraries for this case study:

```python
#Importing the libraries
#standard imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#sklearn standard functions
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error


#standard imports for pytorch
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
from torch.utils.data import DataLoader, TensorDataset
from torch import Tensor
from torch.profiler import profile, record_function, ProfilerActivity 

#torchvision imports
import torchvision
import torchvision.transforms as transforms

#Other imports 
import time
import tqdm as tqdm 
```

### Step 2: Data preparation and preprocessing 

The first step in any machine learning project is to prepare the data. In this step, we will be loading the data, performing data preprocessing, and splitting the data into training and test sets.


```python
import ssl
ssl._create_default_https_context = ssl._create_unverified_context

#Define data pre-processing steps
transform = transforms.Compose(
    [
    #Resize images for (64*64)
    transforms.Resize((64,64)),
    #Converts images into Pytorch tensor 
    #Pytorch tensors are multi-dimensional arrays that can be processed on GPUs
    transforms.ToTensor(), 
    #Normalise the input data 
    #input data is transformed by subtracting the mean and dividing by the standard deviation for each channel. 
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

#Batch size defines the number of samples processed before the model is updated.
batch_size = 40 

#Loading EuraSAT and transform using the defined function 
dataset = torchvision.datasets.EuroSAT(root='./data', 
                                        download=True, transform=transform)

#Data loader creates a PyTorch data loader for a given dataset. 
#The data loader provides an efficient way to iterate over the data in the dataset
#and apply batch processing during training.      
#num_workers: defines the number of threads to use for loading the data. 
#If shuffle=True, the data loader will randomly shuffle the data before each epoch to ensure that the model sees a different set of samples each time it is trained.
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

#Classes -> we have 10 labels 
#'AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River' 'SeaLake'
classes = data_loader.dataset.classes

split=len(dataset.targets)/4
train_len=int(len(dataset.targets)-split)
val_len=int(split) 

#Spliting dataset in 75% training, 25% for testing  
trainset,testset = torch.utils.data.random_split(dataset, [train_len,val_len])

#Create dataloader for training and testing dataset 
train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)
```


### Step 3: Visualizing the data

Let's visualize some images in the dataset.

```python
import os 
import random 
from PIL import Image

ROOT_dir = './data/eurosat/2750'
folders = os.listdir(ROOT_dir)

plt.figure(figsize=(16,10))

for i, label in enumerate(folders):
    plt.subplot(4,5,i+1)
    file_path = os.listdir("{}/{}".format(ROOT_dir,label))
    image_ = Image.open(ROOT_dir+"/"+label+"/"+file_path[random.randint(1,100)])
    plt.imshow(image_)
    plt.title(label)
    plt.axis("off") 
```

<figure title = "CPU tensorboard">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp2_example_data.png?raw=true">
    <figcaption>
    <b>Figure 2: Example classes and images</b>
    </b>
    </figcaption>
    </center>
</figure>


### Step 4: Creating your CNN model for training

Now that we have prepared the data, we can create our CNN model. We will be using the following architecture for our model:

```python
import torch.nn as nn
import torch.nn.functional as F

#Custom class extends the functionality of nn.Module class from PyTorch, 
#which provides the basic building blocks for creating neural networks in PyTorch. 
class Net(nn.Module):
    #Setting up layers in CNN 
    def __init__(self):
        #Calling function from nn.Module
        super().__init__()
        #A 2D convolutional layer with 3 input channels, 6 output, and kernel (filter size) size of 5x5 
        self.conv1 = nn.Conv2d(3, 6, 5)
        #A max-pooling layer with kernel size 2x2 and stride of 2
        self.pool = nn.MaxPool2d(2, 2)
        #Another convolution layer with 6 input channels, 16 output channels, and a kernel size of 5x5 
        self.conv2 = nn.Conv2d(6, 16, 5)
        #Three fully-connected linear layers for processing the output of the second convolution network 
        self.fc1 = nn.Linear(16 * 13 * 13, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    #Define the foward pass of the network i.e. the computation performed on each input tensor. 
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

``` 

We can use the following code to print out the summary of the model:


```python
from torchsummary import summary
summary(Net(), (3,64,64),device='cpu')
```

```python
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 6, 60, 60]             456
         MaxPool2d-2            [-1, 6, 30, 30]               0
            Conv2d-3           [-1, 16, 26, 26]           2,416
         MaxPool2d-4           [-1, 16, 13, 13]               0
            Linear-5                  [-1, 120]         324,600
            Linear-6                   [-1, 84]          10,164
            Linear-7                   [-1, 10]             850
================================================================
Total params: 338,486
Trainable params: 338,486
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.05
Forward/backward pass size (MB): 0.31
Params size (MB): 1.29
Estimated Total Size (MB): 1.65
----------------------------------------------------------------
```
### Step 5: Inspecting CPU/GPU usage with PyTorch Profiler and TensorBoard 

PyTorch includes a simple profiler API that is useful for measuring the training performance and resource utilization of your model. The objective is to target the execution steps that are the most costly in time and memory and visualize the workload distribution between GPUs and CPUs.

Firstly, let's define a function to train the model. This function will be used to train the model for each batch of data. 

```python
def train(model,data,criterion, optimizer,device = device):
    # Copy the data to the device the model is on 
    inputs, labels = data[0].to(device=device), data[1].to(device=device)

    #Predict the output for given input
    outputs = model(inputs)

    #Compute the loss
    loss = criterion(outputs, labels)

    #Clear the previous gradients, compute gradients of all variables wrt loss
    optimizer.zero_grad()

    #Backpropagation, update weights
    loss.backward()

    #Update the parameters
    optimizer.step()
```

Next, we can use the profiler to record the execution steps and save the logs to a file. We can then use TensorBoard to visualize the logs. The profiler includes a number of options to customize the profiling behavior. In this example, we will use the following options: 

* **schedule**: defines the number of steps to wait before starting the profiling, the number of steps to run the profiling for, and the number of steps to repeat the profiling for. In this example, with repeat=4, profiler will record 4 spans, each span consists of 2 wait step, 2 warmup step and 3 active steps. For more information about wait/warmup/active, you can find it [here](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe html#using-profiler-to-analyze-long-running-jobs#using-profiler-to-analyze-long-running-jobs). It is important to note we are not training the whole model in this example, as it would take a long time to run. Instead, we are only training the model for a few steps. 
* **on_trace_ready**: defines the action to take when the profiling is complete. In this example, we will save the profiling logs to a file that can be used by TensorBoard.
* **profile_memory**: enables memory profiling to measure tensor memory allocation/deallocation.  


```python
#GPU ----------------------------
#Initialise model 
device = torch.device('cuda:0') 
model = Net().to(device=device)
#Define loss function 
loss_fn =  nn.CrossEntropyLoss().cuda()#Loss function computes the value between the predicted values and the labels. In this case, we are using Cross-Entropy loss, but many other loss functions are also avaible from nn. Such as focal loss 

#Define optimizer function 
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #Optimizer function aims to reduce the loss function's value by changing the weight vector values through backpropagation in neural networks. We are using Stochastic gradient decent as our optimiser, with learning rate 0.01 and momentum 0.9 

#Set random seed for reproducibility
torch.cuda.manual_seed(42)

#Profiler
with torch.profiler.profile(
       schedule=torch.profiler.schedule(
        wait=2,
        warmup=2,
        active=3,
        repeat=4), 
        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/gpu_profile'),
        profile_memory=True,
        ) as prof:
    for step, batch_data in enumerate(train_loader,0):
        if step >= (2 + 2 + 3) * 4:
            break
        train(model =model , data =batch_data, criterion = loss_fn, optimizer = optimizer,device=device)
        prof.step()

```

```python
#CPU ----------------------------
#Reinitialise model, loss function, optimizer and random seed 
device = torch.device('cpu')
model = Net().to(device=device)
loss_fn =  nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
torch.manual_seed(42)

with torch.profiler.profile(
       schedule=torch.profiler.schedule(
        wait=2,
        warmup=2,
        active=3,
        repeat=4), 
        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/cpu_profile'),
        profile_memory=True,
        ) as prof:
    for step, batch_data in enumerate(train_loader,0):
        if step >= (2 + 2 + 3) * 4:
            break
        train(model =model , data =batch_data, criterion = loss_fn, optimizer = optimizer,device=device)
        prof.step()
```

We can then use TensorBoard to visualize the profiling logs. The following command will launch TensorBoard and open the profiling dashboard.

```python
%load_ext tensorboard
%tensorboard --logdir ./log
```

Or in VSCode, you can press Ctrl+Shift+P and type "Open TensorBoard". Then select the log directory.


The TensorBoard profiling dashboard includes a number of tabs that can be used to visualize the profiling logs. In this example, we will be focusing on the **Overview**

* **Overview**: provides a high-level overview of the profiling results. In this example, the GPU Utilization is low. The details of these metrics are [here](https://github.com/pytorch/kineto/blob/main/tb_plugin/docs/gpu_utilization.md).


<figure title = "GPU tensorboard">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp2_tb_gpu.png?raw=true">
    <figcaption>
    <b>Figure 3: GPU tensorboard</b>
    </b>
    </figcaption>
    </center>
</figure>



<figure title = "CPU tensorboard">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp2_tb_cpu.png?raw=true">
    <figcaption>
    <b>Figure 4: CPU tensorboard</b>
    </b>
    </figcaption>
    </center>
</figure>

As we can see from Figure 1, the GPU Utilization is low. This is because the data and model size is small, and the overhead of transferring the data from CPU to GPU is significant. We cab further investigate the profiling logs in the command line: 

```python
print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10)) 
```

From the profiling logs, we can see that enumerate(dataloader) is the most time-consuming operation. This is because the dataloader is a generator that yields a batch of data at each iteration. Therefore, the time spent on each iteration is the time spent on the dataloader plus the time spent on the training function. In this example, the time spent on the dataloader is significant because the data size is small. Therefore, the time spent on the training function is relatively small.

The effect of small data on GPU and CPU can vary depending on the architecture of the machine, the size of the data, and the computation being performed.In general, when the data size is small, the performance difference between GPU and CPU may not be significant. This is because the overhead of transferring the data from CPU to GPU and vice versa can outweigh the performance gain from using GPU acceleration. Additionally, when the computation is relatively simple, the GPU may not be utilized to its full potential. However, when the data size increases or the computation becomes more complex, the GPU can significantly outperform the CPU. This is because GPUs have a large number of cores designed to perform parallel computation, which makes them well-suited for tasks such as training deep neural networks and processing large amounts of data. Therefore, it's important to carefully consider the size of the data and the computation being performed before deciding to use a GPU or a CPU.


## Case 2: Fine-tuning a pre-trained model on a large dataset 
